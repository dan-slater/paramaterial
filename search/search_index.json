{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Paramaterial Documentation","text":""},{"location":"#about","title":"About","text":"<p>A Python package for parameterizing materials test data. Given a set of experimental measurements, Paramaterial can be used to determine material properties and constitutive model parameters.</p> <ul> <li> <p>Source Code: https://github.com/dan-slater/paramaterial</p> </li> <li> <p>Documentation: https://dan-slater.github.io/paramaterial/</p> </li> <li> <p>PyPI: https://pypi.org/project/paramaterial/</p> </li> <li> <p>Examples: https://github.com/dan-slater/paramaterial-examples</p> </li> </ul> <p>The package was designed to help improve the quality and quantity of data available for materials modeling and simulation. It is hoped that using Paramaterial will help improve repeatability and reproducibility of materials test data analyses, and help to reduce the time and effort required to perform such analyses.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install paramaterial\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>Please see the API reference for details on the toolkit's functions and classes.</p> <p>Usage examples are available in the examples repository. These examples can be downloaded using the <code>download_example</code> function:</p> <pre><code># Download the basic usage example to the current directory\nfrom paramaterial import download_example\ndownload_example('dan_msc_basic_usage_0.1.0')\n# Other examples are also currently available:\n# download_example('dan_msc_cs1_0.1.0')\n# download_example('dan_msc_cs2_0.1.0')\n# download_example('dan_msc_cs3_0.1.0')\n# download_example('dan_msc_cs4_0.1.0')\n</code></pre> <p>The examples include datasets, notebooks, and other assets that showcase the functionality and capabilities of the Paramaterial library. These examples can be downloaded and run locally, providing an interactive way to explore and learn about the library. For more details see the documentation for the <code>download_example</code> function at reference/example.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Please go to the GitHub repository and submit an issue or pull request.</p>"},{"location":"#license","title":"License","text":"<p>Paramaterial is licensed under the MIT license. </p>"},{"location":"reference/aggregating/","title":"Aggregating","text":"<p>This module provides functionalities to make representative curves from data and find statistics for metadata.</p> Functions <ul> <li><code>_generate_filter_permutations(info_table, group_by)</code> - Generates filter permutations for grouping data.</li> <li><code>make_representative_data(ds, info_path, data_dir, repres_col, group_by_keys, interp_by, interp_res, interp_range, group_info_cols)</code> - Creates representative curves from a dataset and saves them to a directory.</li> <li><code>make_representative_info(ds, group_by_keys, group_info_cols)</code> - Creates a table of representative information for each group in a DataSet.</li> </ul>"},{"location":"reference/aggregating/#paramaterial.aggregating.make_representative_data","title":"<code>make_representative_data(ds, info_path, data_dir, repres_col, group_by_keys, interp_by, interp_res=200, interp_range='outer', group_info_cols=None)</code>","text":"<p>Make representative curves of the DataSet and save them to a directory.</p> <p>This function takes a DataSet, groups it by specific keys, and creates representative curves. The curves are then saved to a specified directory. It is useful for generating aggregated data curves that represent groups of similar tests.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>DataSet</code> <p>The DataSet to make representative curves from.</p> required <code>info_path</code> <code>str</code> <p>The path to the info file where the representative information will be saved.</p> required <code>data_dir</code> <code>str</code> <p>The directory to save the representative curves to.</p> required <code>group_by_keys</code> <code>List[str]</code> <p>The info columns to group the tests by.</p> required <code>repres_col</code> <code>str</code> <p>The data column to aggregate for the y-axis of the representative curves.</p> required <code>interp_by</code> <code>str</code> <p>The data column to interpolate for the x-axis of the representative curves.</p> required <code>interp_res</code> <code>int</code> <p>The resolution of the interpolation.</p> <code>200</code> <code>interp_range</code> <code>Union[str, Tuple[float, float]]</code> <p>Can be either \"outer\", \"inner\", or a tuple of floats, defining the domain on the x-axis for</p> <code>'outer'</code> <code>interpolation</code> <ul> <li>If \"outer\", the domain is defined by the smallest minimum and the largest maximum values of the interpolation column in the representative subset.</li> <li>If \"inner\", the domain is defined by the largest minimum and the smallest maximum values of the interpolation column in the representative subset.</li> <li>If a tuple, the domain is directly defined by the values within the tuple.</li> </ul> required <code>group_info_cols</code> <code>Optional[List[str]]</code> <p>The info categories to include in the aggregated info_table.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <p>Imagine you have performed a series of stress tests on different materials at various temperatures. You have collected all the data in a DataSet and want to create representative stress-strain curves for each combination of material and temperature. Here's how you can use this function:</p> <pre><code>&gt;&gt;&gt; import paramaterial as pam\n&gt;&gt;&gt; ds = pam.DataSet('info/test_info.csv','data/tests')  # Load your dataset\n&gt;&gt;&gt; pam.make_representative_data(ds, 'info/representative_info.xlsx', 'data/representative_curves',\n&gt;&gt;&gt;                              repres_col='Stress_MPa', group_by_keys=['material', 'temperature'],\ninterp_by='Strain')\n</code></pre> <p>This will create representative curves for each material and temperature group, saving them to the specified directory and information to an Excel file.</p> Source code in <code>paramaterial\\aggregating.py</code> <pre><code>def make_representative_data(ds: DataSet, info_path: str, data_dir: str, repres_col: str, group_by_keys: List[str],\ninterp_by: str, interp_res: int = 200,\ninterp_range: Union[str, Tuple[float, float]] = 'outer',\ngroup_info_cols: Optional[List[str]] = None):\n\"\"\"Make representative curves of the DataSet and save them to a directory.\n     This function takes a DataSet, groups it by specific keys, and creates representative curves. The curves are\n     then saved to a specified directory. It is useful for generating aggregated data curves that represent groups of\n     similar tests.\n     Args:\n         ds: The DataSet to make representative curves from.\n         info_path: The path to the info file where the representative information will be saved.\n         data_dir: The directory to save the representative curves to.\n         group_by_keys: The info columns to group the tests by.\n         repres_col: The data column to aggregate for the y-axis of the representative curves.\n         interp_by: The data column to interpolate for the x-axis of the representative curves.\n         interp_res: The resolution of the interpolation.\n         interp_range: Can be either \"outer\", \"inner\", or a tuple of floats, defining the domain on the x-axis for\n         interpolation:\n            - If \"outer\", the domain is defined by the smallest minimum and the largest maximum values of the\n            interpolation column in the representative subset.\n            - If \"inner\", the domain is defined by the largest minimum and the smallest maximum values of the\n            interpolation column in the representative subset.\n            - If a tuple, the domain is directly defined by the values within the tuple.\n         group_info_cols: The info categories to include in the aggregated info_table.\n     Returns:\n         None\n     Examples:\n        Imagine you have performed a series of stress tests on different materials at various temperatures. You have\n        collected all the data in a DataSet and want to create representative stress-strain curves for each\n        combination of material and temperature. Here's how you can use this function:\n        &gt;&gt;&gt; import paramaterial as pam\n        &gt;&gt;&gt; ds = pam.DataSet('info/test_info.csv','data/tests')  # Load your dataset\n        &gt;&gt;&gt; pam.make_representative_data(ds, 'info/representative_info.xlsx', 'data/representative_curves',\n        &gt;&gt;&gt;                              repres_col='Stress_MPa', group_by_keys=['material', 'temperature'],\n        interp_by='Strain')\n        This will create representative curves for each material and temperature group, saving them to the specified\n        directory and information to an Excel file.\n    \"\"\"\nif not os.path.exists(data_dir):\nos.makedirs(data_dir)\nvalue_lists = [ds.info_table[col].unique() for col in group_by_keys]\n# make a dataset filter for each representative curve\nsubset_filters = []\nfor i in range(len(value_lists[0])):  # i\nsubset_filters.append({group_by_keys[0]: value_lists[0][i]})\nfor i in range(1, len(group_by_keys)):  # i\nnew_filters = []\nfor fltr in subset_filters:  # j\nfor value in value_lists[i]:  # k\nnew_filter = fltr.copy()\nnew_filter[group_by_keys[i]] = value\nnew_filters.append(new_filter)\nsubset_filters = new_filters\n# make list of repres_ids and initialise info table for the representative data\nrepres_ids = [f'repres_id_{i + 1:0&gt;4}' for i in range(len(subset_filters))]\nrepr_info_table = pd.DataFrame(columns=['repres_id'] + group_by_keys)\n# make representative curves and take means of info table columns\nfor repres_id, subset_filter in zip(repres_ids, subset_filters):\n# get representative subset\nrepres_subset = ds.subset(subset_filter)\nif repres_subset.info_table.empty:\ncontinue\n# add row to repr_info_table\nrepr_info_table = pd.concat([repr_info_table, pd.DataFrame(\n{'repres_id': [repres_id], **subset_filter, 'nr averaged': [len(repres_subset)]})])\n# add means of group info columns to repr_info_table\nif group_info_cols is not None:\nfor col in group_info_cols:\ndf_col = repres_subset.info_table[col]\nrepr_info_table.loc[repr_info_table['repres_id'] == repres_id, '' + col] = df_col.mean()\nrepr_info_table.loc[repr_info_table['repres_id'] == repres_id, 'std_' + col] = df_col.std()\nrepr_info_table.loc[\nrepr_info_table['repres_id'] == repres_id, 'upstd_' + col] = df_col.mean() + df_col.std()\nrepr_info_table.loc[\nrepr_info_table['repres_id'] == repres_id, 'downstd_' + col] = df_col.mean() - df_col.std()\nrepr_info_table.loc[repr_info_table['repres_id'] == repres_id, 'max_' + col] = df_col.max()\nrepr_info_table.loc[repr_info_table['repres_id'] == repres_id, 'min_' + col] = df_col.min()\n# find minimum of maximum interp_by vals in subset\nif interp_range == 'outer':\nmin_interp_val = min([min(dataitem.data[interp_by]) for dataitem in repres_subset])\nmax_interp_val = max([max(dataitem.data[interp_by]) for dataitem in repres_subset])\nelif interp_range == 'inner':\nmin_interp_val = max([min(dataitem.data[interp_by]) for dataitem in repres_subset])\nmax_interp_val = min([max(dataitem.data[interp_by]) for dataitem in repres_subset])\nelif type(interp_range) == tuple:\nmin_interp_val = interp_range[0]\nmax_interp_val = interp_range[1]\nelse:\nraise ValueError(f'interp_range must be \"outer\", \"inner\" or a tuple, not {interp_range}')\n# make monotonically increasing vector to interpolate by\ninterp_vec = np.linspace(min_interp_val, max_interp_val, interp_res)\n# make interpolated data for averaging, staring at origin\ninterp_data = pd.DataFrame(data={interp_by: interp_vec})\nfor n, dataitem in enumerate(repres_subset):\n# drop columns and rows outside interp range\ndata = dataitem.data[[interp_by, repres_col]].reset_index(drop=True)\ndata = data[(data[interp_by] &lt;= max_interp_val) &amp; (data[interp_by] &gt;= min_interp_val)]\n# interpolate the repr_by column and add to interp_data\n# add 0 to start of data to ensure interpolation starts at origin\ninterp_data[f'interp_{repres_col}_{n}'] = np.interp(interp_vec, data[interp_by].tolist(),\ndata[repres_col].tolist())\n# make representative data from stats of interpolated data\ninterp_data = interp_data.drop(columns=[interp_by])\nrepr_data = pd.DataFrame({f'{interp_by}': interp_vec})\nrepr_data[f'{repres_col}'] = interp_data.mean(axis=1)\nrepr_data[f'std_{repres_col}'] = interp_data.std(axis=1)\nrepr_data[f'up_std_{repres_col}'] = repr_data[f'{repres_col}'] + repr_data[f'std_{repres_col}']\nrepr_data[f'down_std_{repres_col}'] = repr_data[f'{repres_col}'] - repr_data[f'std_{repres_col}']\nrepr_data[f'up_2std_{repres_col}'] = repr_data[f'{repres_col}'] + 2 * repr_data[f'std_{repres_col}']\nrepr_data[f'down_2std_{repres_col}'] = repr_data[f'{repres_col}'] - 2 * repr_data[f'std_{repres_col}']\nrepr_data[f'up_3std_{repres_col}'] = repr_data[f'{repres_col}'] + 3 * repr_data[f'std_{repres_col}']\nrepr_data[f'down_3std_{repres_col}'] = repr_data[f'{repres_col}'] - 3 * repr_data[f'std_{repres_col}']\nrepr_data[f'min_{repres_col}'] = interp_data.min(axis=1)\nrepr_data[f'max_{repres_col}'] = interp_data.max(axis=1)\nrepr_data[f'q1_{repres_col}'] = interp_data.quantile(0.25, axis=1)\nrepr_data[f'q3_{repres_col}'] = interp_data.quantile(0.75, axis=1)\n# write the representative data and info\nrepr_data.to_csv(os.path.join(data_dir, f'{repres_id}.csv'), index=False)\nrepr_info_table.to_excel(info_path, index=False)\n</code></pre>"},{"location":"reference/aggregating/#paramaterial.aggregating.make_representative_info","title":"<code>make_representative_info(ds, group_by_keys, group_info_cols=None)</code>","text":"<p>Make a table of representative info for each group in a DataSet.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>DataSet</code> <p>DataSet to make representative info from.</p> required <code>group_by_keys</code> <code>List[str]</code> <p>Columns to group by and make representative info for.</p> required <code>group_info_cols</code> <code>List[str]</code> <p>Columns to include in representative info table.</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A pandas DataFrame containing the representative information table.</p> <p>Examples:</p> <p>To create a summary table that includes specific mechanical properties like Elastic Modulus (E), Proof Stress (PS), Ultimate Tensile Strength (UTS), for each temperature and material type:</p> <pre><code>&gt;&gt;&gt; import paramaterial as pam\n&gt;&gt;&gt; table = pam.make_representative_info(ds, group_by_keys=['temperature', 'material'], group_info_cols=['E', 'PS', 'UTS'])\n&gt;&gt;&gt; print(table.head())\n</code></pre> <p>The result will be a DataFrame containing representative information for each group, including the mean, standard deviation, maximum, minimum, and 1st and 3rd quartiles of the specified columns.</p> Source code in <code>paramaterial\\aggregating.py</code> <pre><code>def make_representative_info(ds: DataSet, group_by_keys: List[str], group_info_cols: List[str] = None) -&gt; pd.DataFrame:\n\"\"\"Make a table of representative info for each group in a DataSet.\n    Args:\n        ds: DataSet to make representative info from.\n        group_by_keys: Columns to group by and make representative info for.\n        group_info_cols: Columns to include in representative info table.\n    Returns:\n        A pandas DataFrame containing the representative information table.\n    Examples:\n        To create a summary table that includes specific mechanical properties like Elastic Modulus (E), Proof Stress\n        (PS), Ultimate Tensile Strength (UTS), for each temperature and material type:\n        &gt;&gt;&gt; import paramaterial as pam\n        &gt;&gt;&gt; table = pam.make_representative_info(ds, group_by_keys=['temperature', 'material'], group_info_cols=['E', 'PS', 'UTS'])\n        &gt;&gt;&gt; print(table.head())\n        The result will be a DataFrame containing representative information for each group, including the mean, standard\n        deviation, maximum, minimum, and 1st and 3rd quartiles of the specified columns.\n    \"\"\"\nsubset_filters = []\nvalue_lists = [ds.info_table[col].unique() for col in group_by_keys]\nfor i in range(len(value_lists[0])):\nsubset_filters.append({group_by_keys[0]: [value_lists[0][i]]})\nfor i in range(1, len(group_by_keys)):\nnew_filters = []\nfor fltr in subset_filters:\nfor value in value_lists[i]:\nnew_filter = fltr.copy()\nnew_filter[group_by_keys[i]] = [value]\nnew_filters.append(new_filter)\nsubset_filters = new_filters\n# make list of repres_ids and initialise info table for the representative data\nrepres_ids = [f'repres_id_{i + 1:0&gt;4}' for i in range(len(subset_filters))]\nrepr_info_table = pd.DataFrame(columns=['repres_id'] + group_by_keys)\nfor fltr, repres_id in zip(subset_filters, repres_ids):\n# get representative subset\nrepr_subset = ds.subset(fltr)\nif repr_subset.info_table.empty:\ncontinue\n# add row to repr_info_table\nrepr_info_table = pd.concat(\n[repr_info_table, pd.DataFrame({'repres_id': [repres_id], **fltr, 'nr averaged': [len(repr_subset)]})])\n# add means of group info columns to repr_info_table\nif group_info_cols is not None:\nfor col in group_info_cols:\ndf_col = repr_subset.info_table[col]\nrepr_info_table.loc[repr_info_table['repres_id'] == repres_id, '' + col] = df_col.mean()\nrepr_info_table.loc[repr_info_table['repres_id'] == repres_id, 'std_' + col] = df_col.std()\nrepr_info_table.loc[\nrepr_info_table['repres_id'] == repres_id, 'upstd_' + col] = df_col.mean() + df_col.std()\nrepr_info_table.loc[\nrepr_info_table['repres_id'] == repres_id, 'downstd_' + col] = df_col.mean() - df_col.std()\nrepr_info_table.loc[repr_info_table['repres_id'] == repres_id, 'max_' + col] = df_col.max()\nrepr_info_table.loc[repr_info_table['repres_id'] == repres_id, 'min_' + col] = df_col.min()\nreturn repr_info_table\n</code></pre>"},{"location":"reference/example/","title":"Example","text":"<p>The example module is designed to facilitate the downloading and setup of predefined examples for the Paramaterial library.</p> Function <ul> <li><code>download_example</code>: A function to download a specified example, extract its content, and save it to a designated directory.</li> </ul> Currently Available Examples <ul> <li>'dan_msc_basic_usage_0.1.0'</li> <li>'dan_msc_cs1_0.1.0'</li> <li>'dan_msc_cs2_0.1.0'</li> <li>'dan_msc_cs3_0.1.0'</li> <li>'dan_msc_cs4_0.1.0'</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Download the 'dan_msc_basic_usage_0.1.0' example to the current directory\n&gt;&gt;&gt; from paramaterial import download_example\n&gt;&gt;&gt; download_example('dan_msc_basic_usage_0.1.0')\n</code></pre> About the Example Repository <p>The examples are hosted in a GitHub repository and include datasets, notebooks, and other assets that showcase the functionality and capabilities of the Paramaterial library. These examples can be downloaded and run locally, providing an interactive way to explore and learn about the library.</p> <p>The <code>download_example</code> function allows users to fetch any of the available examples by name. It takes care of downloading and extracting the data, info, and notebook files to the specified directory.</p>"},{"location":"reference/example/#paramaterial.example.download_example","title":"<code>download_example(example_name, to_directory='./')</code>","text":"<p>Download and extract an example from the Paramaterial example repository.</p> <p>Parameters:</p> Name Type Description Default <code>example_name</code> <code>str</code> <p>The name of the example to download. Must be one of the predefined examples available in the Paramaterial example repository. Examples include:</p> <ul> <li>'dan_msc_basic_usage_0.1.0'</li> <li>'dan_msc_cs1_0.1.0'</li> <li>'dan_msc_cs2_0.1.0'</li> <li>'dan_msc_cs3_0.1.0'</li> <li>'dan_msc_cs4_0.1.0'</li> </ul> required <code>to_directory</code> <code>str</code> <p>The directory to download and extract the example to.</p> <code>'./'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified example_name is not recognized.</p> <code>Exception</code> <p>If an error occurs during the download or extraction process.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; download_example('dan_msc_basic_usage_0.1.0')\nExample dan_msc_basic_usage_0.1.0 downloaded to ./\n</code></pre> Source code in <code>paramaterial\\example.py</code> <pre><code>def download_example(example_name: str, to_directory: str = './'):\n\"\"\"Download and extract an example from the Paramaterial example repository.\n    Args:\n        example_name: The name of the example to download. Must be one of the predefined\n            examples available in the Paramaterial example repository. Examples include:\n            - 'dan_msc_basic_usage_0.1.0'\n            - 'dan_msc_cs1_0.1.0'\n            - 'dan_msc_cs2_0.1.0'\n            - 'dan_msc_cs3_0.1.0'\n            - 'dan_msc_cs4_0.1.0'\n        to_directory: The directory to download and extract the example to.\n    Raises:\n        ValueError: If the specified example_name is not recognized.\n        Exception: If an error occurs during the download or extraction process.\n    Examples:\n        &gt;&gt;&gt; download_example('dan_msc_basic_usage_0.1.0')\n        Example dan_msc_basic_usage_0.1.0 downloaded to ./\n    \"\"\"\n# Check if the example name is recognized\nif example_name not in EXAMPLE_NAMES:\nraise ValueError(f'Example name {example_name} not recognized. '\nf'Existing example names are: {\", \".join(EXAMPLE_NAMES)}.')\n# Create the output directory if it doesn't exist\nos.makedirs(to_directory, exist_ok=True)\n# Download the example tarball\nurl = f'{BASE_URL}/{example_name}.tar.gz'\ntarball_path = f'{example_name}.tar.gz'\nresponse = requests.get(url, stream=True)\nif response.status_code == 200:\nwith open(tarball_path, 'wb') as file:\nfor chunk in response.iter_content(chunk_size=8192):\nfile.write(chunk)\nelse:\nraise Exception(f\"Download tarball error occurred: {response.status_code}\")\n# Extract the example tarball\ntry:\nshutil.unpack_archive(tarball_path, to_directory)\n# os.rename(extracted_dir, os.path.join(to_directory, example_name))\nexcept FileNotFoundError as fnf_error:\nprint(f\"No file: {fnf_error}\")\nexcept Exception as err:\nprint(f\"An error occurred during extraction: {err}\")\n# Clean up the downloaded tarball\ntry:\n# pass\nos.remove(tarball_path)\nexcept Exception as e:\nprint(f\"An error occurred while deleting file : {e}\")\nprint(f\"Example {example_name} downloaded to {to_directory}\")\n</code></pre>"},{"location":"reference/modelling/","title":"Modelling","text":"<p>Module for modelling materials test data.</p> <p>This module provides functionalities to parameterize mechanical test data by fitting constitutive models to the data. It includes classes to fit mathematical models to materials test data and predict material behavior. The module integrates with the <code>plug</code> module for data handling.</p> Classes <ul> <li>ModelSet: Acts as a model DataSet, used to fit, collect, and predict using constitutive models.</li> </ul>"},{"location":"reference/modelling/#paramaterial.modelling.ModelSet","title":"<code>ModelSet</code>","text":"<p>Class that acts as a model DataSet, providing functionalities to fit, collect, and predict constitutive models for material behavior.</p> <p>This class is designed to fit mathematical models to mechanical test data and make predictions based on the fitted parameters. It integrates with the DataSet and DataItem classes for handling data.</p> <p>Attributes:</p> Name Type Description <code>model_func</code> <code>Callable</code> <p>A function defining the mathematical model to be fitted. It should accept an array of                    x-values and a tuple of variables and parameters, and return an array of y-values.</p> <code>variable_names</code> <code>List[str]</code> <p>List of variable names that may be used in the model_func.</p> <code>param_names</code> <code>List[str]</code> <p>List of parameter names for the model.</p> <code>bounds</code> <code>List[Tuple[float, float]]</code> <p>Bounds for the model parameters.</p> <code>initial_guess</code> <code>Tuple[float]</code> <p>Initial guess for the model parameters.</p> <code>sample_range</code> <code>Tuple[float, float]</code> <p>Range of samples for fitting.</p> <code>sample_size</code> <code>int</code> <p>Size of the sample data.</p> <code>model_id_key</code> <code>str</code> <p>Key for the model ID.</p> <code>fitting_table</code> <code>pd.DataFrame</code> <p>Pandas DataFrame storing the fitting results.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import paramaterial as pam\n&gt;&gt;&gt; from paramaterial import DataSet, DataItem, ModelSet\n&gt;&gt;&gt; model_func = pam.models.linear\n&gt;&gt;&gt; param_names = ['E', 's_y']\n&gt;&gt;&gt; ms = ModelSet(model_func=model_func, param_names=param_names)\n&gt;&gt;&gt; ds = DataSet(info_path='info.csv', data_dir='data/')\n&gt;&gt;&gt; ms.fit_to(ds, x_key='strain', y_key='stress', sample_range=(0.0, 0.05), sample_size=100)\n&gt;&gt;&gt; prediction_ds = ms.predict(xmin=0, xmax=0.05)\n</code></pre> Source code in <code>paramaterial\\modelling.py</code> <pre><code>class ModelSet:\n\"\"\"\n    Class that acts as a model DataSet, providing functionalities to fit, collect, and predict constitutive models\n    for material behavior.\n    This class is designed to fit mathematical models to mechanical test data and make predictions based on the fitted\n    parameters. It integrates with the DataSet and DataItem classes for handling data.\n    Attributes:\n        model_func (Callable): A function defining the mathematical model to be fitted. It should accept an array of\n                               x-values and a tuple of variables and parameters, and return an array of y-values.\n        variable_names (List[str]): List of variable names that may be used in the model_func.\n        param_names (List[str]): List of parameter names for the model.\n        bounds (List[Tuple[float, float]]): Bounds for the model parameters.\n        initial_guess (Tuple[float]): Initial guess for the model parameters.\n        sample_range (Tuple[float, float]): Range of samples for fitting.\n        sample_size (int): Size of the sample data.\n        model_id_key (str): Key for the model ID.\n        fitting_table (pd.DataFrame): Pandas DataFrame storing the fitting results.\n    Examples:\n        &gt;&gt;&gt; import paramaterial as pam\n        &gt;&gt;&gt; from paramaterial import DataSet, DataItem, ModelSet\n        &gt;&gt;&gt; model_func = pam.models.linear\n        &gt;&gt;&gt; param_names = ['E', 's_y']\n        &gt;&gt;&gt; ms = ModelSet(model_func=model_func, param_names=param_names)\n        &gt;&gt;&gt; ds = DataSet(info_path='info.csv', data_dir='data/')\n        &gt;&gt;&gt; ms.fit_to(ds, x_key='strain', y_key='stress', sample_range=(0.0, 0.05), sample_size=100)\n        &gt;&gt;&gt; prediction_ds = ms.predict(xmin=0, xmax=0.05)\n    \"\"\"\ndef __init__(self,\nmodel_func: Callable[[np.ndarray, Tuple[float]], np.ndarray],\nvar_names: List[str],\nparam_names: List[str],\nbounds: List[Tuple[float, float]] = None,\ninitial_guess: Tuple[float] = None,\nsample_range: Tuple[float, float] = (None, None),\nsample_size: int = 50,\nmodel_id_key: str = 'model_id',\nscipy_func: str = 'minimize',\n):\nself.model_func = model_func\nself.variable_names = var_names  # Updated name\nself.param_names = param_names\nself.bounds = bounds\nself.initial_guess = initial_guess if initial_guess else [0.0] * len(param_names)\nself.sample_range = sample_range\nself.sample_size = sample_size\nself.model_id_key = model_id_key\nself.scipy_func = scipy_func\n# self.fitting_table: pd.DataFrame = pd.DataFrame(\n#     columns=[model_id_key] + ['var_' + var_name for var_name in var_names] +\n#             ['param_' + param_name for param_name in param_names] + ['error'])\nself.fitting_table: pd.DataFrame = pd.DataFrame(\ncolumns=[model_id_key] + ['var_' + var_name for var_name in var_names] +\n[param_name for param_name in param_names] + ['error'])\ndef fit_to(self, ds: DataSet, x_key: str, y_key: str, sample_range: Tuple[float, float] = (None, None),\nsample_size: int = 50, **scipy_method_kwargs):\n\"\"\"\n    Fits the model to a given DataSet using the specified x and y keys for the independent and dependent variables.\n    Args:\n        ds (DataSet): The DataSet containing the data to be fitted.\n        x_key (str): The key for the independent variable (e.g., 'strain').\n        y_key (str): The key for the dependent variable (e.g., 'stress').\n        sample_range (Tuple[float, float], optional): The range of samples for fitting. Defaults to (None, None).\n        sample_size (int, optional): The size of the sample data. Defaults to 50.\n        **scipy_method_kwargs: Additional keyword arguments to pass to the SciPy optimization method.\n    Returns:\n        None: Updates the fitting_table attribute with the fitting results.\n    Examples:\n        &gt;&gt;&gt; model_func = lambda x, args: args[0] * x + args[1]  # Example linear model\n        &gt;&gt;&gt; model_set = ModelSet(model_func, var_names=['a', 'b'], param_names=['slope', 'intercept'])\n        &gt;&gt;&gt; ds = DataSet()  # Assume this is a pre-loaded DataSet with 'strain' and 'stress' columns\n        &gt;&gt;&gt; model_set.fit_to(ds, x_key='strain', y_key='stress')\n    \"\"\"\n# Set the keys\nself.x_col = x_key\nself.y_col = y_key\n# Call the existing fit method\nself.fit_items(ds, sample_range, sample_size, self.scipy_func, **scipy_method_kwargs)\ndef predict(self, x_range: Optional[Tuple[float, float, float]] = None,\nxmin: Optional[float] = None,\nxmax: Optional[float] = None,\ninfo_table: Optional[pd.DataFrame] = None,\nmodel_id_key: str = 'model_id'):\n\"\"\"\n        Makes predictions based on the fitted model over a specified x-range.\n        Args:\n            x_range (Tuple[float, float, float], optional): Range for prediction in the form (xmin, xmax, step).\n            xmin (float, optional): Minimum x value for prediction.\n            xmax (float, optional): Maximum x value for prediction.\n            info_table (pd.DataFrame, optional): Information table containing parameters and variables.\n            model_id_key (str, optional): Key for the model ID. Defaults to 'model_id'.\n        Returns:\n            DataSet: A DataSet containing the predicted values.\n        Examples:\n            &gt;&gt;&gt; x_range = (0, 10, 0.1)  # Define x range for prediction\n            &gt;&gt;&gt; predicted_ds = model_set.predict(x_range=x_range)\n        \"\"\"\n# If x_range is not provided, check for xmin and xmax\nif x_range is None:\nif xmin is None or xmax is None:\nx_range = (0, 0.01, 0.0001)  # Example default value\nelse:\nx_range = (xmin, xmax, 0.0001)  # Example step value, adjust as needed\nreturn self.predict_ds(x_range, info_table, model_id_key)\ndef _sample_data(self, di: DataItem) -&gt; Tuple[np.ndarray, np.ndarray]:\nsample_range = self.sample_range\nsample_size = self.sample_size\nx_data = di.data[self.x_col].values\ny_data = di.data[self.y_col].values\nif sample_range[0] is not None and sample_range[1] is not None:\nmask = (x_data &gt; sample_range[0]) &amp; (x_data &lt; sample_range[1])\nx_data, y_data = x_data[mask], y_data[mask]\nsampling_stride = max(int(len(x_data) / sample_size), 1)\nx_data, y_data = x_data[::sampling_stride], y_data[::sampling_stride]\nreturn x_data, y_data\ndef _objective_function(self, params: Tuple[float, ...], di: DataItem) -&gt; float:\nx_data, y_data = self._sample_data(di)\nif self.variable_names is not None:\nvariables = tuple(di.info[var_name] for var_name in self.variable_names)\nelse:\nvariables = ()\nvariables_and_params = variables + tuple(params)\ny_model = self.model_func(x_data, variables_and_params)\nreturn _error_norm(y_data, y_model)\ndef _fit_item(self, di: DataItem, scipy_method: str, **scipy_method_kwargs) -&gt; op.OptimizeResult:\nreturn _call_scipy_method(scipy_method=scipy_method, initial_guess=self.initial_guess, bounds=self.bounds,\nobjective_function=self._objective_function, storage_object=di, **scipy_method_kwargs)\ndef fit_items(self, ds: DataSet, sample_range: Tuple[float, float] = (None, None), sample_size: int = 50,\nscipy_method: str = 'minimize', **scipy_method_kwargs):\nself.sample_range = sample_range\nself.sample_size = sample_size\n# fit each DataItem and add a row to fitting_table for each\nfitting_dfs = []\npad = int(np.log10(len(ds))) + 1\nfor i, di in enumerate(ds):\nmodel_id = f'{self.model_id_key}_{i+1:0{pad}}'\n# run optimisation\nfitting_result = self._fit_item(di, scipy_method, **scipy_method_kwargs)\n# extract results\nparams = fitting_result.x\nerror = fitting_result.fun\nvariables = di.info[self.variable_names]\n# add 'var_' prefix to variable names\nvariables.index = 'var_' + variables.index\n# add 'param_' prefix to param names\n# params = pd.Series(params, index='param_' + pd.Series(self.param_names))\nparams = pd.Series(params, index=pd.Series(self.param_names))\n# add row to fitting_table\n# concatenate data\ndata = np.hstack([model_id, variables, params, error, di.info.to_list()]).reshape(1, -1)\n# define columns\ncolumns = self.fitting_table.columns.tolist() + di.info.index.to_list()\n# create DataFrame and append\nfitting_dfs.append(pd.DataFrame(data, columns=columns))\n# concatenate fitting_dfs into fitting_table\nself.fitting_table = pd.concat(fitting_dfs)\ndef predict_ds(self, x_range: Tuple[float, float, float], info_table: Optional[pd.DataFrame] = None,\nmodel_id_key: str = 'model_id'):\nif info_table is None:\ninfo_table = self.fitting_table\n# generate DataItems for each row of info_table\nmodel_items = []\nfor model_id in info_table[model_id_key].to_list():\ndi_info = info_table.loc[info_table[model_id_key] == model_id, :].squeeze()\n# extract variables and optimised params from info_table\nvariables_keys = self.variable_names if self.variable_names else []\nvariables_keys = ['var_' + var_key for var_key in variables_keys]\n# params_keys = ['param_' + param_key for param_key in self.param_names]\nparams_keys = [param_key for param_key in self.param_names]\nvariables_and_params = di_info[variables_keys + params_keys].to_list()\n# generate model data and create DataItem\nx_model = np.arange(*x_range)\ny_model = self.model_func(x_model, variables_and_params)\ndata = pd.DataFrame({self.x_col: x_model, self.y_col: y_model})\nmodel_items.append(DataItem(model_id, data=data, info=di_info))\n# create DataSet from model_items and return\nds = DataSet(test_id_key=model_id_key)\nds.data_items = model_items\nds.info_table = info_table\nreturn ds\n</code></pre>"},{"location":"reference/modelling/#paramaterial.modelling.ModelSet.fit_to","title":"<code>fit_to(ds, x_key, y_key, sample_range=(None, None), sample_size=50, **scipy_method_kwargs)</code>","text":"<p>Fits the model to a given DataSet using the specified x and y keys for the independent and dependent variables.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>DataSet</code> <p>The DataSet containing the data to be fitted.</p> required <code>x_key</code> <code>str</code> <p>The key for the independent variable (e.g., 'strain').</p> required <code>y_key</code> <code>str</code> <p>The key for the dependent variable (e.g., 'stress').</p> required <code>sample_range</code> <code>Tuple[float, float]</code> <p>The range of samples for fitting. Defaults to (None, None).</p> <code>(None, None)</code> <code>sample_size</code> <code>int</code> <p>The size of the sample data. Defaults to 50.</p> <code>50</code> <code>**scipy_method_kwargs</code> <p>Additional keyword arguments to pass to the SciPy optimization method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>None</code> <p>Updates the fitting_table attribute with the fitting results.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model_func = lambda x, args: args[0] * x + args[1]  # Example linear model\n&gt;&gt;&gt; model_set = ModelSet(model_func, var_names=['a', 'b'], param_names=['slope', 'intercept'])\n&gt;&gt;&gt; ds = DataSet()  # Assume this is a pre-loaded DataSet with 'strain' and 'stress' columns\n&gt;&gt;&gt; model_set.fit_to(ds, x_key='strain', y_key='stress')\n</code></pre> Source code in <code>paramaterial\\modelling.py</code> <pre><code>def fit_to(self, ds: DataSet, x_key: str, y_key: str, sample_range: Tuple[float, float] = (None, None),\nsample_size: int = 50, **scipy_method_kwargs):\n\"\"\"\nFits the model to a given DataSet using the specified x and y keys for the independent and dependent variables.\nArgs:\n    ds (DataSet): The DataSet containing the data to be fitted.\n    x_key (str): The key for the independent variable (e.g., 'strain').\n    y_key (str): The key for the dependent variable (e.g., 'stress').\n    sample_range (Tuple[float, float], optional): The range of samples for fitting. Defaults to (None, None).\n    sample_size (int, optional): The size of the sample data. Defaults to 50.\n    **scipy_method_kwargs: Additional keyword arguments to pass to the SciPy optimization method.\nReturns:\n    None: Updates the fitting_table attribute with the fitting results.\nExamples:\n    &gt;&gt;&gt; model_func = lambda x, args: args[0] * x + args[1]  # Example linear model\n    &gt;&gt;&gt; model_set = ModelSet(model_func, var_names=['a', 'b'], param_names=['slope', 'intercept'])\n    &gt;&gt;&gt; ds = DataSet()  # Assume this is a pre-loaded DataSet with 'strain' and 'stress' columns\n    &gt;&gt;&gt; model_set.fit_to(ds, x_key='strain', y_key='stress')\n\"\"\"\n# Set the keys\nself.x_col = x_key\nself.y_col = y_key\n# Call the existing fit method\nself.fit_items(ds, sample_range, sample_size, self.scipy_func, **scipy_method_kwargs)\n</code></pre>"},{"location":"reference/modelling/#paramaterial.modelling.ModelSet.predict","title":"<code>predict(x_range=None, xmin=None, xmax=None, info_table=None, model_id_key='model_id')</code>","text":"<p>Makes predictions based on the fitted model over a specified x-range.</p> <p>Parameters:</p> Name Type Description Default <code>x_range</code> <code>Tuple[float, float, float]</code> <p>Range for prediction in the form (xmin, xmax, step).</p> <code>None</code> <code>xmin</code> <code>float</code> <p>Minimum x value for prediction.</p> <code>None</code> <code>xmax</code> <code>float</code> <p>Maximum x value for prediction.</p> <code>None</code> <code>info_table</code> <code>pd.DataFrame</code> <p>Information table containing parameters and variables.</p> <code>None</code> <code>model_id_key</code> <code>str</code> <p>Key for the model ID. Defaults to 'model_id'.</p> <code>'model_id'</code> <p>Returns:</p> Name Type Description <code>DataSet</code> <p>A DataSet containing the predicted values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; x_range = (0, 10, 0.1)  # Define x range for prediction\n&gt;&gt;&gt; predicted_ds = model_set.predict(x_range=x_range)\n</code></pre> Source code in <code>paramaterial\\modelling.py</code> <pre><code>def predict(self, x_range: Optional[Tuple[float, float, float]] = None,\nxmin: Optional[float] = None,\nxmax: Optional[float] = None,\ninfo_table: Optional[pd.DataFrame] = None,\nmodel_id_key: str = 'model_id'):\n\"\"\"\n    Makes predictions based on the fitted model over a specified x-range.\n    Args:\n        x_range (Tuple[float, float, float], optional): Range for prediction in the form (xmin, xmax, step).\n        xmin (float, optional): Minimum x value for prediction.\n        xmax (float, optional): Maximum x value for prediction.\n        info_table (pd.DataFrame, optional): Information table containing parameters and variables.\n        model_id_key (str, optional): Key for the model ID. Defaults to 'model_id'.\n    Returns:\n        DataSet: A DataSet containing the predicted values.\n    Examples:\n        &gt;&gt;&gt; x_range = (0, 10, 0.1)  # Define x range for prediction\n        &gt;&gt;&gt; predicted_ds = model_set.predict(x_range=x_range)\n    \"\"\"\n# If x_range is not provided, check for xmin and xmax\nif x_range is None:\nif xmin is None or xmax is None:\nx_range = (0, 0.01, 0.0001)  # Example default value\nelse:\nx_range = (xmin, xmax, 0.0001)  # Example step value, adjust as needed\nreturn self.predict_ds(x_range, info_table, model_id_key)\n</code></pre>"},{"location":"reference/models/","title":"Models","text":""},{"location":"reference/models/#paramaterial.models.apply_ZH_regression","title":"<code>apply_ZH_regression(ds, flow_stress_key='flow_stress_MPa', ZH_key='ZH_parameter', group_by=None)</code>","text":"<p>Do a linear regression for LnZ vs flow stress. #todo link</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>DataSet</code> <p>DataSet to be fitted.</p> required <code>flow_stress_key</code> <code>str</code> <p>Info key for the flow stress value.</p> <code>'flow_stress_MPa'</code> <code>ZH_key</code> <code>str</code> <p>Info key for the ZH parameter value.</p> <code>'ZH_parameter'</code> <code>group_by</code> <code>Union[str, List[str]]</code> <p>Info key(s) to group by.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataSet</code> <p>The DataSet with the Zener-Holloman parameter and regression parameters added to the info table.</p> Source code in <code>paramaterial\\models.py</code> <pre><code>def apply_ZH_regression(ds: DataSet, flow_stress_key: str = 'flow_stress_MPa', ZH_key: str = 'ZH_parameter',\ngroup_by: Union[str, List[str]] = None) -&gt; DataSet:\n\"\"\"Do a linear regression for LnZ vs flow stress. #todo link\n    Args:\n        ds: DataSet to be fitted.\n        flow_stress_key: Info key for the flow stress value.\n        ZH_key: Info key for the ZH parameter value.\n        group_by: Info key(s) to group by.\n    Returns:\n        The DataSet with the Zener-Holloman parameter and regression parameters added to the info table.\n    \"\"\"\nassert flow_stress_key in ds.info_table.columns, f'flow_stress_key {flow_stress_key} not in info table'\n# make dataset filters for unique combinations of group_by keys\nif group_by is not None:\nif isinstance(group_by, str):\ngroup_by = [group_by]\nsubset_filters = []\nvalue_lists = [ds.info_table[col].unique() for col in group_by]\nfor i in range(len(value_lists[0])):\nsubset_filters.append({group_by[0]: [value_lists[0][i]]})\nfor i in range(1, len(group_by)):\nnew_filters = []\nfor fltr in subset_filters:\nfor value in value_lists[i]:\nnew_filter = fltr.copy()\nnew_filter[group_by[i]] = [value]\nnew_filters.append(new_filter)\nsubset_filters = new_filters\ngroups = []\nfor fltr in subset_filters:\ngroup_ds = ds.subset(fltr)\ngroups.append(group_ds)\nelse:\ngroups = [ds]\n# apply regression to each group\nfor group_ds in groups:\ninfo_table = group_ds.info_table.copy()\ninfo_table['lnZ'] = np.log(info_table[ZH_key].values.astype(np.float64))\nresult = curve_fit(lambda x, m, c: m*x + c, info_table['lnZ'], info_table[flow_stress_key])\ninfo_table['lnZ_fit_m'] = result[0][0]\ninfo_table['lnZ_fit_c'] = result[0][1]\ninfo_table['lnZ_fit'] = info_table['lnZ_fit_m']*info_table['lnZ'] + info_table['lnZ_fit_c']\ninfo_table['lnZ_fit_residual'] = info_table['lnZ_fit'] - info_table[flow_stress_key]\ninfo_table['lnZ_fit_r2'] = 1 - np.sum(info_table['lnZ_fit_residual']**2)/np.sum(\n(info_table[flow_stress_key] - np.mean(info_table[flow_stress_key]))**2)\ninfo_table['ZH_fit'] = np.exp(info_table['lnZ_fit'])\ninfo_table['ZH_fit_error'] = info_table['ZH_fit'] - info_table[ZH_key]\ninfo_table['ZH_fit_error_percent'] = info_table['ZH_fit_error']/info_table[ZH_key]\ngroup_ds.info_table = info_table\ngroup_info_tables = [group_ds.info_table for group_ds in groups]\ninfo_table = pd.concat(group_info_tables)\nds.info_table = info_table\nreturn ds\n</code></pre>"},{"location":"reference/models/#paramaterial.models.calculate_ZH_parameter","title":"<code>calculate_ZH_parameter(di, temperature_key='temperature_K', rate_key='rate_s-1', Q_key='Q_activation', gas_constant=8.1345, ZH_key='ZH_parameter')</code>","text":"<p>Calculate the Zener-Holloman parameter using</p> \\[ Z = \\dot{\\varepsilon} \\exp \\left(\\frac{Q}{RT}\\right) \\] <p>where \\(\\dot{\\varepsilon}\\) is the strain rate, \\(Q\\) is the activation energy, \\(R\\) is the gas constant, and \\(T\\) is the temperature.</p> <p>Parameters:</p> Name Type Description Default <code>di</code> <code>DataItem</code> <p>DataItem object with \\(\\dot{\\varepsilon}\\), \\(Q\\), \\(R\\), and \\(T\\) in info.</p> required <code>temperature_key</code> <code>str</code> <p>Info key for mean temperature</p> <code>'temperature_K'</code> <code>rate_key</code> <code>str</code> <p>Info key for mean strain-rate rate</p> <code>'rate_s-1'</code> <code>Q_key</code> <code>str</code> <p>Info key for activation energy</p> <code>'Q_activation'</code> <code>gas_constant</code> <code>float</code> <p>Universal gas constant</p> <code>8.1345</code> <code>ZH_key</code> <code>str</code> <p>Key for Zener-Holloman parameter</p> <code>'ZH_parameter'</code> Source code in <code>paramaterial\\models.py</code> <pre><code>def calculate_ZH_parameter(di: DataItem, temperature_key: str = 'temperature_K', rate_key: str = 'rate_s-1',\nQ_key: str = 'Q_activation', gas_constant: float = 8.1345,\nZH_key: str = 'ZH_parameter') -&gt; DataItem:\n\"\"\"Calculate the Zener-Holloman parameter using\n    $$\n    Z = \\\\dot{\\\\varepsilon} \\\\exp \\\\left(\\\\frac{Q}{RT}\\\\right)\n    $$\n    where $\\\\dot{\\\\varepsilon}$ is the strain rate, $Q$ is the activation energy, $R$ is the gas constant,\n    and $T$ is the temperature.\n    Args:\n        di: DataItem object with $\\\\dot{\\\\varepsilon}$, $Q$, $R$, and $T$ in info.\n        temperature_key: Info key for mean temperature\n        rate_key: Info key for mean strain-rate rate\n        Q_key: Info key for activation energy\n        gas_constant: Universal gas constant\n        ZH_key: Key for Zener-Holloman parameter\n    Returns: DataItem with Zener-Holloman parameter added to info.\n    \"\"\"\ndi.info[ZH_key] = di.info[rate_key]*np.exp(di.info[Q_key]/(gas_constant*di.info[temperature_key]))\nreturn di\n</code></pre>"},{"location":"reference/models/#paramaterial.models.iso_return_map","title":"<code>iso_return_map(yield_stress_func, return_vec='stress')</code>","text":"<p>Wrapper for a yield function that describes the plastic behaviour.</p> <p>Parameters:</p> Name Type Description Default <code>yield_stress_func</code> <code>Callable</code> <p>Yield stress function.</p> required <code>return_vec</code> <code>str</code> <p>Return vector. Must be one of 'stress', 'plastic strain', 'accumulated plastic strain'.</p> <code>'stress'</code> Source code in <code>paramaterial\\models.py</code> <pre><code>def iso_return_map(yield_stress_func: Callable, return_vec: str = 'stress'):\n\"\"\"Wrapper for a yield function that describes the plastic behaviour.\n    Args:\n        yield_stress_func: Yield stress function.\n        return_vec: Return vector. Must be one of 'stress', 'plastic strain', 'accumulated plastic strain'.\n    Returns: A function that gives the return_vec (usually stress) as a function of strain.\n    \"\"\"\n@wraps(yield_stress_func)\ndef wrapper(\nx: np.ndarray,\nmat_params\n):\ny = np.zeros(x.shape)  # predicted stress\nx_p = np.zeros(x.shape)  # plastic strain\naps = np.zeros(x.shape)  # accumulated plastic strain\ny_yield: callable = yield_stress_func(mat_params)  # yield stress\nE = mat_params[0]  # elastic modulus\nif not np.isclose(x[0], 0):\ny_trial_0 = E*(x[1])\nf_trial_0 = np.abs(y_trial_0) - y_yield(0)\nif f_trial_0 &lt;= 0:\ny[0] = E*x[0]\nelse:\nd_aps = op.root(lambda d: f_trial_0 - d*E - y_yield(d) + y_yield(0), 0).x[0]\ny[0] = y_trial_0*(1 - d_aps*E/np.abs(y_trial_0))\nfor i in range(len(x) - 1):\ny_trial = E*(x[i + 1] - x_p[i])\nf_trial = np.abs(y_trial) - y_yield(aps[i])\nif f_trial &lt;= 0:\ny[i + 1] = y_trial\nx_p[i + 1] = x_p[i]\naps[i + 1] = aps[i]\nelse:\nd_aps = op.root(\nlambda d: f_trial - d*E - y_yield(aps[i] + d) + y_yield(aps[i]),\naps[i]\n).x[0]\ny[i + 1] = y_trial*(1 - d_aps*E/np.abs(y_trial))\nx_p[i + 1] = x_p[i] + np.sign(y_trial)*d_aps\naps[i + 1] = aps[i] + d_aps\nif return_vec == 'stress':\nreturn y\nelif return_vec == 'plastic strain':\nreturn x_p\nelif return_vec == 'accumulated plastic strain':\nreturn aps\nelse:\nreturn None\nreturn wrapper\n</code></pre>"},{"location":"reference/models/#paramaterial.models.linear","title":"<code>linear(mat_params)</code>","text":"<p>Linear isotropic hardening yield function.</p> Source code in <code>paramaterial\\models.py</code> <pre><code>@iso_return_map\ndef linear(mat_params):\n\"\"\"Linear isotropic hardening yield function.\"\"\"\nE, s_y, K = mat_params\nreturn lambda a: s_y + K*a\n</code></pre>"},{"location":"reference/models/#paramaterial.models.make_ZH_regression_table","title":"<code>make_ZH_regression_table(ds, flow_stress_key='flow_stress_MPa', rate_key='rate_s-1', temperature_key='temperature_K', calculate=True, group_by=None)</code>","text":"<p>Make a table of the Zener-Holloman regression parameters for each group.</p> Source code in <code>paramaterial\\models.py</code> <pre><code>def make_ZH_regression_table(ds: DataSet, flow_stress_key: str = 'flow_stress_MPa', rate_key: str = 'rate_s-1',\ntemperature_key: str = 'temperature_K', calculate: bool = True,\ngroup_by: Union[str, List[str]] = None) -&gt; pd.DataFrame:\n\"\"\"Make a table of the Zener-Holloman regression parameters for each group.\"\"\"\nif calculate:\nds = ds.apply(calculate_ZH_parameter, rate_key=rate_key, temperature_key=temperature_key)\nds = apply_ZH_regression(ds, flow_stress_key=flow_stress_key, group_by=group_by)\ntable = ds.info_table[['ZH_fit_group', 'lnZ_fit_m', 'lnZ_fit_c', 'lnZ_fit_r2']]\ntable.columns = ['Group', 'Slope', 'Intercept', 'R2']\ntable = table.drop_duplicates().reset_index(drop=True)\nreturn table\n</code></pre>"},{"location":"reference/models/#paramaterial.models.perfect","title":"<code>perfect(mat_params)</code>","text":"<p>Perfect plasticity yield function, no hardening.</p> Source code in <code>paramaterial\\models.py</code> <pre><code>@iso_return_map\ndef perfect(mat_params):\n\"\"\"Perfect plasticity yield function, no hardening.\"\"\"\nE, s_y = mat_params\nreturn lambda a: s_y\n</code></pre>"},{"location":"reference/models/#paramaterial.models.plot_ZH_regression","title":"<code>plot_ZH_regression(ds, flow_stress_key='flow_stress_MPa', rate_key='rate_s-1', temperature_key='temperature_K', calculate=True, figsize=(6, 4), ax=None, cmap='plasma', styler=None, plot_legend=True, group_by=None, color_by=None, marker_by=None, linestyle_by=None, scatter_kwargs=None, fit_kwargs=None, eq_hscale=0.1)</code>","text":"<p>Plot the Zener-Holloman regression of the flow stress vs. temperature.</p> Source code in <code>paramaterial\\models.py</code> <pre><code>def plot_ZH_regression(ds: DataSet, flow_stress_key: str = 'flow_stress_MPa', rate_key: str = 'rate_s-1',\ntemperature_key: str = 'temperature_K', calculate: bool = True,\nfigsize: Tuple[float, float] = (6, 4),\nax: plt.Axes = None, cmap: str = 'plasma', styler: Styler = None, plot_legend: bool = True,\ngroup_by: Union[str, List[str]] = None, color_by: str = None, marker_by: str = None,\nlinestyle_by: str = None,\nscatter_kwargs: Dict[str, Any] = None, fit_kwargs: Dict[str, Any] = None, eq_hscale=0.1):\n\"\"\"Plot the Zener-Holloman regression of the flow stress vs. temperature.\"\"\"\n# configure_plt_formatting()\nif ax is None:\nfig, ax = plt.subplots(1, 1, figsize=figsize)\nif styler is None:\nstyler = Styler(color_by=color_by, color_by_label=color_by, cmap=cmap, marker_by=marker_by,\nmarker_by_label=marker_by, linestyle_by=linestyle_by, linestyle_by_label=linestyle_by\n).style_to(ds)\n# Calculate ZH parameter\nif calculate:\nds = ds.apply(calculate_ZH_parameter, rate_key=rate_key, temperature_key=temperature_key)\n# make a scatter plot of lnZ vs flow stress using the styler\nfor di in ds:\nupdated_scatter_kwargs = styler.curve_formatters(di)\nupdated_scatter_kwargs.update(scatter_kwargs) if scatter_kwargs is not None else None\nupdated_scatter_kwargs.pop('linestyle') if 'linestyle' in updated_scatter_kwargs else None\nupdated_scatter_kwargs.update({'color': 'k'}) if color_by is None else None\nax.scatter(np.log(di.info['ZH_parameter']), di.info[flow_stress_key], **updated_scatter_kwargs)\nax.set_prop_cycle(None)  # reset ax color cycle\n# make dataset filters for unique combinations of group_by keys\nif group_by is not None:\nif isinstance(group_by, str):\ngroup_by = [group_by]\nsubset_filters = []\nvalue_lists = [ds.info_table[col].unique() for col in group_by]\nfor i in range(len(value_lists[0])):\nsubset_filters.append({group_by[0]: [value_lists[0][i]]})\nfor i in range(1, len(group_by)):\nnew_filters = []\nfor fltr in subset_filters:\nfor value in value_lists[i]:\nnew_filter = fltr.copy()\nnew_filter[group_by[i]] = [value]\nnew_filters.append(new_filter)\nsubset_filters = new_filters\ngroups = []\nfor fltr in subset_filters:\ngroup_ds = ds.subset(fltr)\ngroup_ds = apply_ZH_regression(group_ds, flow_stress_key=flow_stress_key) if calculate else group_ds\ngroups.append(group_ds)\nelse:\ngroup_ds = apply_ZH_regression(ds, flow_stress_key=flow_stress_key) if calculate else ds\ngroups = [group_ds]\n# plot the regression lines\nfor group_ds in groups:\nx = np.linspace(group_ds.info_table['lnZ'].min(), group_ds.info_table['lnZ'].max(), 10)\ndi = group_ds[0]\ny = di.info['lnZ_fit_m']*x + di.info['lnZ_fit_c']\nupdated_fit_kwargs = styler.curve_formatters(di)\nupdated_fit_kwargs.pop('marker') if 'marker' in updated_fit_kwargs else None\nupdated_fit_kwargs.update(fit_kwargs) if fit_kwargs is not None else None\nax.plot(x, y, **updated_fit_kwargs)\n# add the legend\nhandles = styler.legend_handles(ds)\nif len(handles) &gt; 0 and plot_legend:\nax.legend(handles=handles, loc='best', handletextpad=0.05, markerfirst=False)  # , labelspacing=0.1)\nax.get_legend().set_zorder(2000)\nax.set_xlabel('lnZ')\nax.set_ylabel('Flow Stress (MPa)')\nax.set_title('Zener-Holloman Regression')\n# add annotation to bottom right of axes with the regression equation\nheights = reversed([0.05 + eq_hscale*i for i in range(len(groups))])\nfor group_ds, height in zip(groups, heights):\ndi = group_ds[0]\ncolor = styler.color_dict[di.info[color_by]] if color_by is not None else 'k'\ninfo = di.info\nax.text(0.95, height,\nf'y = {info[\"lnZ_fit_m\"]:.2f}x + {info[\"lnZ_fit_c\"]:.2f} | r$^2$ = {info[\"lnZ_fit_r2\"]:.2f}',\nhorizontalalignment='right',\nverticalalignment='bottom', transform=ax.transAxes,\nbbox=dict(facecolor=color, alpha=0.2, edgecolor='none', boxstyle='round,pad=0.2'))\nreturn ax\n</code></pre>"},{"location":"reference/models/#paramaterial.models.quadratic","title":"<code>quadratic(mat_params)</code>","text":"<p>Quadratic isotropic hardening yield function.</p> Source code in <code>paramaterial\\models.py</code> <pre><code>@iso_return_map\ndef quadratic(mat_params):\n\"\"\"Quadratic isotropic hardening yield function.\"\"\"\nE, s_y, Q = mat_params\nreturn lambda a: s_y + E*(a - Q*a**2)\n</code></pre>"},{"location":"reference/models/#paramaterial.models.ramberg","title":"<code>ramberg(mat_params)</code>","text":"<p>Ramberg-Osgood isotropic hardening yield function.</p> Source code in <code>paramaterial\\models.py</code> <pre><code>@iso_return_map\ndef ramberg(mat_params):\n\"\"\"Ramberg-Osgood isotropic hardening yield function.\"\"\"\nE, s_y, C, n = mat_params\nreturn lambda a: s_y + C*(np.sign(a)*(np.abs(a))**n)\n</code></pre>"},{"location":"reference/models/#paramaterial.models.voce","title":"<code>voce(mat_params)</code>","text":"<p>Exponential isotropic hardening yield function.</p> Source code in <code>paramaterial\\models.py</code> <pre><code>@iso_return_map\ndef voce(mat_params):\n\"\"\"Exponential isotropic hardening yield function.\"\"\"\nE, s_y, s_u, d = mat_params\nreturn lambda a: s_y + (s_u - s_y)*(1 - np.exp(-d*a))\n</code></pre>"},{"location":"reference/plotting/","title":"Plotting","text":"<p>Module containing the plotting functions for the ds class.</p>"},{"location":"reference/plotting/#paramaterial.plotting.Styler","title":"<code>Styler</code>  <code>dataclass</code>","text":"<p>A class for storing plotting styles for a dataset.</p> Source code in <code>paramaterial\\plotting.py</code> <pre><code>@dataclass\nclass Styler:\n\"\"\"A class for storing plotting styles for a dataset.\"\"\"\ncolor_by: Optional[str] = None\nlinestyle_by: Optional[str] = None\nmarker_by: Optional[str] = None\nwidth_by: Optional[str] = None\ncolor_by_label: Optional[str] = None\nlinestyle_by_label: Optional[str] = None\nmarker_by_label: Optional[str] = None\nwidth_by_label: Optional[str] = None\ncbar: Optional[bool] = False\ncolor_norm: Optional[plt.Normalize] = None\ncbar_label: Optional[str] = None\ncmap: str = 'plasma'\nhandles: Optional[List[mpatches.Patch]] = None\nlinestyles: List[str] = field(default_factory=lambda: ['-', '--', ':', '-.'])\nmarkers: List[str] = field(\ndefault_factory=lambda: ['s', 'H', 'd', 'v', 'D', 'p', 'X', 'o', 'd', 'h', 'H', '8', 'P', 'x'])\ncolor_dict: Optional[Dict[Union[str, int, float], str]] = None\nlinestyle_dict: Optional[Dict[Union[str, int, float], str]] = None\nmarker_dict: Optional[Dict[Union[str, int, float], str]] = None\nplot_kwargs: Dict[str, Any] = field(default_factory=lambda: dict())\nstyled_ds: Optional[DataSet] = None\ndef __post_init__(self):\nself.plot_kwargs['legend'] = False\nself.plot_kwargs.update({'markeredgecolor': 'white', 'markersize': 8})\n# todo: use pandas in-built color bar\n# if self.cbar:\n#     self.plot_kwargs['cmap'] = self.cmap\n#     self.plot_kwargs['norm'] = self.color_norm\n#     self.plot_kwargs['colorbar'] = True\n#     self.plot_kwargs['colorbar_label'] = self.cbar_label\ndef style_to(self, ds: DataSet):\n\"\"\"Format the styles to match the dataset.\"\"\"\nself.styled_ds = copy.deepcopy(ds)\nif self.color_by is not None:\ncolor_vals = ds.info_table[self.color_by].unique()\nif all(str(x).isnumeric() for x in color_vals):\nif self.color_norm is None:\nself.color_norm = plt.Normalize(color_vals.min(), color_vals.max())\nself.color_dict = {x: plt.cm.get_cmap(self.cmap)(self.color_norm(x)) for x in color_vals}\nelse:\nself.color_norm = plt.Normalize(0, len(color_vals))\nself.color_dict = {x: plt.cm.get_cmap(self.cmap)(self.color_norm(i)) for i, x in enumerate(color_vals)}\nif self.linestyle_by is not None:\nlinestyle_vals = ds.info_table[self.linestyle_by].unique().tolist()\nwhile len(self.linestyles) &lt; len(linestyle_vals):\nself.linestyles.extend(self.linestyles)\nself.linestyle_dict = dict(zip(linestyle_vals, self.linestyles))\nif self.marker_by is not None:\nmarker_vals = ds.info_table[self.marker_by].unique().tolist()\nwhile len(self.markers) &lt; len(marker_vals):\nself.markers.extend(self.markers)\nself.marker_dict = dict(zip(marker_vals, self.markers))\nreturn self\ndef curve_formatters(self, di: DataItem) -&gt; Dict[str, Any]:\n\"\"\"Return the curve formatters for the dataitem curve.\"\"\"\nif self.styled_ds is None:\nraise ValueError('The styler must be styled to a ds before plotting.')\n# configure_plt_formatting()\nformatters = dict()\nif self.color_by is not None:\nif di.info[self.color_by] not in self.color_dict.keys():\ntry:\nformatters['color'] = self.color_dict[float(di.info[self.color_by])]\nexcept ValueError:\nformatters['color'] = plt.gca()._get_lines.get_next_color()\nelse:\nformatters['color'] = self.color_dict[di.info[self.color_by]]\nif all(str(x).isnumeric() for x in self.color_dict.keys()):\nformatters['zorder'] = di.info[self.color_by]\nelse:\nformatters['color'] = plt.gca()._get_lines.get_next_color()\nif self.linestyle_by is not None:\nformatters['linestyle'] = self.linestyle_dict[di.info[self.linestyle_by]]\nif self.marker_by is not None:\nformatters['marker'] = self.marker_dict[di.info[self.marker_by]]\nreturn {k: v for k, v in formatters.items() if v is not None}\ndef legend_handles(self, ds: Optional[DataSet] = None) -&gt; List[mpatches.Patch]:\n\"\"\"Return the legend handles for the dataset plot.\"\"\"\nhandles = list()\nif ds is None:\nds = self.styled_ds\nif len(ds) == 0:\nreturn handles\nif self.color_by_label is not None:\nhandles.append(mpatches.Patch(label=self.color_by_label.title(), alpha=0))\nif self.color_by is not None:\nfor color_val in ds.info_table[self.color_by].unique():\nif color_val not in self.color_dict.keys():\ncolor_val = float(color_val)\nhandles.append(Line2D([], [], label=color_val, color=self.color_dict[color_val], marker='o', ls=''))\nif self.linestyle_by_label is not None:\nhandles.append(mpatches.Patch(label='\\n' + self.linestyle_by_label.title(), alpha=0))\nif self.linestyle_by is not None:\nfor ls_val in ds.info_table[self.linestyle_by].unique():\nhandles.append(Line2D([], [], label=ls_val, ls=self.linestyle_dict[ls_val], c='k', marker=''))\nif self.marker_by_label is not None:\nhandles.append(mpatches.Patch(label='\\n' + self.marker_by_label.title(), alpha=0))\nif self.marker_by is not None:\nfor marker_val in ds.info_table[self.marker_by].unique():\nhandles.append(Line2D([], [], label=marker_val, marker=self.marker_dict[marker_val], c='k', ls=''))\nreturn handles\n</code></pre>"},{"location":"reference/plotting/#paramaterial.plotting.Styler.curve_formatters","title":"<code>curve_formatters(di)</code>","text":"<p>Return the curve formatters for the dataitem curve.</p> Source code in <code>paramaterial\\plotting.py</code> <pre><code>def curve_formatters(self, di: DataItem) -&gt; Dict[str, Any]:\n\"\"\"Return the curve formatters for the dataitem curve.\"\"\"\nif self.styled_ds is None:\nraise ValueError('The styler must be styled to a ds before plotting.')\n# configure_plt_formatting()\nformatters = dict()\nif self.color_by is not None:\nif di.info[self.color_by] not in self.color_dict.keys():\ntry:\nformatters['color'] = self.color_dict[float(di.info[self.color_by])]\nexcept ValueError:\nformatters['color'] = plt.gca()._get_lines.get_next_color()\nelse:\nformatters['color'] = self.color_dict[di.info[self.color_by]]\nif all(str(x).isnumeric() for x in self.color_dict.keys()):\nformatters['zorder'] = di.info[self.color_by]\nelse:\nformatters['color'] = plt.gca()._get_lines.get_next_color()\nif self.linestyle_by is not None:\nformatters['linestyle'] = self.linestyle_dict[di.info[self.linestyle_by]]\nif self.marker_by is not None:\nformatters['marker'] = self.marker_dict[di.info[self.marker_by]]\nreturn {k: v for k, v in formatters.items() if v is not None}\n</code></pre>"},{"location":"reference/plotting/#paramaterial.plotting.Styler.legend_handles","title":"<code>legend_handles(ds=None)</code>","text":"<p>Return the legend handles for the dataset plot.</p> Source code in <code>paramaterial\\plotting.py</code> <pre><code>def legend_handles(self, ds: Optional[DataSet] = None) -&gt; List[mpatches.Patch]:\n\"\"\"Return the legend handles for the dataset plot.\"\"\"\nhandles = list()\nif ds is None:\nds = self.styled_ds\nif len(ds) == 0:\nreturn handles\nif self.color_by_label is not None:\nhandles.append(mpatches.Patch(label=self.color_by_label.title(), alpha=0))\nif self.color_by is not None:\nfor color_val in ds.info_table[self.color_by].unique():\nif color_val not in self.color_dict.keys():\ncolor_val = float(color_val)\nhandles.append(Line2D([], [], label=color_val, color=self.color_dict[color_val], marker='o', ls=''))\nif self.linestyle_by_label is not None:\nhandles.append(mpatches.Patch(label='\\n' + self.linestyle_by_label.title(), alpha=0))\nif self.linestyle_by is not None:\nfor ls_val in ds.info_table[self.linestyle_by].unique():\nhandles.append(Line2D([], [], label=ls_val, ls=self.linestyle_dict[ls_val], c='k', marker=''))\nif self.marker_by_label is not None:\nhandles.append(mpatches.Patch(label='\\n' + self.marker_by_label.title(), alpha=0))\nif self.marker_by is not None:\nfor marker_val in ds.info_table[self.marker_by].unique():\nhandles.append(Line2D([], [], label=marker_val, marker=self.marker_dict[marker_val], c='k', ls=''))\nreturn handles\n</code></pre>"},{"location":"reference/plotting/#paramaterial.plotting.Styler.style_to","title":"<code>style_to(ds)</code>","text":"<p>Format the styles to match the dataset.</p> Source code in <code>paramaterial\\plotting.py</code> <pre><code>def style_to(self, ds: DataSet):\n\"\"\"Format the styles to match the dataset.\"\"\"\nself.styled_ds = copy.deepcopy(ds)\nif self.color_by is not None:\ncolor_vals = ds.info_table[self.color_by].unique()\nif all(str(x).isnumeric() for x in color_vals):\nif self.color_norm is None:\nself.color_norm = plt.Normalize(color_vals.min(), color_vals.max())\nself.color_dict = {x: plt.cm.get_cmap(self.cmap)(self.color_norm(x)) for x in color_vals}\nelse:\nself.color_norm = plt.Normalize(0, len(color_vals))\nself.color_dict = {x: plt.cm.get_cmap(self.cmap)(self.color_norm(i)) for i, x in enumerate(color_vals)}\nif self.linestyle_by is not None:\nlinestyle_vals = ds.info_table[self.linestyle_by].unique().tolist()\nwhile len(self.linestyles) &lt; len(linestyle_vals):\nself.linestyles.extend(self.linestyles)\nself.linestyle_dict = dict(zip(linestyle_vals, self.linestyles))\nif self.marker_by is not None:\nmarker_vals = ds.info_table[self.marker_by].unique().tolist()\nwhile len(self.markers) &lt; len(marker_vals):\nself.markers.extend(self.markers)\nself.marker_dict = dict(zip(marker_vals, self.markers))\nreturn self\n</code></pre>"},{"location":"reference/plotting/#paramaterial.plotting.dataset_plot","title":"<code>dataset_plot(ds, styler=None, ax=None, fill_between=None, plot_legend=True, handletextpad=0.05, labelspacing=0.1, **kwargs)</code>","text":"<p>Make a single combined plot from the data of every dataitem in the dataset using pandas.DataFrame.plot.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>DataSet</code> <p>The dataset to plot.</p> required <code>styler</code> <code>Optional[Styler]</code> <p>The styler to use for the plot.</p> <code>None</code> <code>ax</code> <code>Optional[plt.Axes]</code> <p>The axis to plot on.</p> <code>None</code> <code>fill_between</code> <code>Optional[Tuple[str, str]]</code> <p>A tuple of the two columns in the data to fill between.</p> <code>None</code> <code>plot_legend</code> <code>bool</code> <p>Whether to plot the legend.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the pandas.DataFrame.plot function.</p> <code>{}</code> Source code in <code>paramaterial\\plotting.py</code> <pre><code>def dataset_plot(\nds: DataSet,\nstyler: Optional[Styler] = None,\nax: Optional[plt.Axes] = None,\nfill_between: Optional[Tuple[str, str]] = None,\nplot_legend: bool = True,\nhandletextpad: float = 0.05,\nlabelspacing: float = 0.1,\n**kwargs\n) -&gt; plt.Axes:\n\"\"\"Make a single combined plot from the data of every dataitem in the dataset using pandas.DataFrame.plot.\n    Args:\n        ds: The dataset to plot.\n        styler: The styler to use for the plot.\n        ax: The axis to plot on.\n        fill_between: A tuple of the two columns in the data to fill between.\n        plot_legend: Whether to plot the legend.\n        **kwargs: Additional keyword arguments to pass to the pandas.DataFrame.plot function.\n    Returns: The axis the plot was made on.\n    \"\"\"\nif ax is None:\nfig, (ax) = plt.subplots(1, 1, figsize=kwargs.get('figsize', (4, 3)))\nkwargs['ax'] = ax\nif ax.get_legend() is not None and plot_legend:\nax.get_legend().remove()\nif styler is None:\nstyler = Styler()\nstyler.style_to(ds)\nkwargs = {**styler.plot_kwargs, **kwargs}\n# plot the dataitems\nfor di in ds:\n# plot the curve\nax = di.data.plot(**styler.curve_formatters(di), **kwargs)\n# fill between curves\nif fill_between is not None:\nax.fill_between(di.data[kwargs['x']], di.data[fill_between[0]], di.data[fill_between[1]], alpha=0.2,\n**styler.curve_formatters(di))\n# add the legend\nhandles = styler.legend_handles(ds)\nif len(handles) &gt; 0 and plot_legend:\nax.legend(handles=handles, loc='best', frameon=False, markerfirst=True, handletextpad=handletextpad,\nlabelspacing=labelspacing)  # handletextpad=0.05)  # , labelspacing=0.1)\nax.get_legend().set_zorder(2000)\n# colorbar\nif styler.cbar and plot_legend:\nsm = plt.cm.ScalarMappable(cmap=styler.cmap, norm=styler.color_norm)\nsm.set_array([])\ncbar = plt.colorbar(sm, ax=kwargs['ax'], fraction=0.046, pad=0.04)\ncbar.set_label(styler.cbar_label) if styler.cbar_label is not None else None\ncbar.ax.yaxis.set_ticks_position('right')\ncbar.ax.yaxis.set_label_position('right')\nreturn ax\n</code></pre>"},{"location":"reference/plotting/#paramaterial.plotting.dataset_subplots","title":"<code>dataset_subplots(ds, shape, rows_by, cols_by, row_vals, col_vals, styler=None, axs=None, figsize=(9, 6), sharex='col', sharey='row', wspace=0.05, hspace=0.05, row_titles=None, col_titles=None, plot_titles=None, subplot_legend=True, subplot_cbar=False, subplots_adjust=0.0, **kwargs)</code>","text":"<p>Plot a dataset as a grid of subplots, split by the 'rows_by' and 'cols_by' columns in the info_table.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>DataSet</code> <p>The dataset to plot.</p> required <code>shape</code> <code>Tuple[int, int]</code> <p>The shape of the grid of subplots.</p> required <code>rows_by</code> <code>str</code> <p>The column in the info_table to split the rows by.</p> required <code>cols_by</code> <code>str</code> <p>The column in the info_table to split the columns by.</p> required <code>row_vals</code> <code>List[List[Any]]</code> <p>The values of the rows to plot.</p> required <code>col_vals</code> <code>List[List[Any]]</code> <p>The values of the columns to plot.</p> required <code>styler</code> <code>Optional[Styler]</code> <p>The styler to use for the plot.</p> <code>None</code> <code>axs</code> <code>Optional[np.ndarray]</code> <p>The axes to plot on.</p> <code>None</code> <code>figsize</code> <code>Tuple[float, float]</code> <p>The size of the figure.</p> <code>(9, 6)</code> <code>sharex</code> <code>str</code> <p>Whether to share the x axis between subplots.</p> <code>'col'</code> <code>sharey</code> <code>str</code> <p>Whether to share the y axis between subplots.</p> <code>'row'</code> <code>wspace</code> <code>float</code> <p>The width space between subplots.</p> <code>0.05</code> <code>hspace</code> <code>float</code> <p>The height space between subplots.</p> <code>0.05</code> <code>row_titles</code> <code>Optional[List[str]]</code> <p>The titles of the rows.</p> <code>None</code> <code>col_titles</code> <code>Optional[List[str]]</code> <p>The titles of the columns.</p> <code>None</code> <code>plot_titles</code> <code>Optional[List[str]]</code> <p>The titles of the subplots.</p> <code>None</code> <code>subplot_legend</code> <code>bool</code> <p>Whether to plot the legend in each subplot.</p> <code>True</code> <code>subplot_cbar</code> <code>bool</code> <p>Whether to plot the colorbar in each subplot.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the pandas.DataFrame.plot function.</p> <code>{}</code> Source code in <code>paramaterial\\plotting.py</code> <pre><code>def dataset_subplots(\nds: DataSet,\nshape: Tuple[int, int],\nrows_by: str,\ncols_by: str,\nrow_vals: List[List[Any]],\ncol_vals: List[List[Any]],\nstyler: Optional[Styler] = None,\naxs: Optional[np.ndarray] = None,\nfigsize: Tuple[float, float] = (9, 6),\nsharex: str = 'col',\nsharey: str = 'row',\nwspace: float = 0.05,\nhspace: float = 0.05,\nrow_titles: Optional[List[str]] = None,\ncol_titles: Optional[List[str]] = None,\nplot_titles: Optional[List[str]] = None,\nsubplot_legend: bool = True,\nsubplot_cbar: bool = False,\nsubplots_adjust: float = 0.0,\n**kwargs\n) -&gt; plt.Axes:\n\"\"\"Plot a dataset as a grid of subplots, split by the 'rows_by' and 'cols_by' columns in the info_table.\n    Args:\n        ds: The dataset to plot.\n        shape: The shape of the grid of subplots.\n        rows_by: The column in the info_table to split the rows by.\n        cols_by: The column in the info_table to split the columns by.\n        row_vals: The values of the rows to plot.\n        col_vals: The values of the columns to plot.\n        styler: The styler to use for the plot.\n        axs: The axes to plot on.\n        figsize: The size of the figure.\n        sharex: Whether to share the x axis between subplots.\n        sharey: Whether to share the y axis between subplots.\n        wspace: The width space between subplots.\n        hspace: The height space between subplots.\n        row_titles: The titles of the rows.\n        col_titles: The titles of the columns.\n        plot_titles: The titles of the subplots.\n        subplot_legend: Whether to plot the legend in each subplot.\n        subplot_cbar: Whether to plot the colorbar in each subplot.\n        **kwargs: Additional keyword arguments to pass to the pandas.DataFrame.plot function.\n    Returns: The axes the plot was made on.\n    \"\"\"\nif axs is None:\nfig, axs = plt.subplots(shape[0], shape[1], figsize=figsize, sharex=sharex, sharey=sharey)\nfig.subplots_adjust(wspace=wspace, hspace=hspace)\nif shape[0] == 1 and shape[1] == 1:\naxs = np.array([[axs]])\nelif shape[0] == 1:\naxs = np.array([axs])\nelif shape[1] == 1:\naxs = np.array([[ax] for ax in axs])\nif styler is None:\nstyler = Styler()\n# set the titles of the rows and columns\nif row_titles is not None:\nfor ax, row_title in zip(axs[:, 0], row_titles):\nax.annotate(row_title, xy=(0, 0.5), xytext=(-ax.yaxis.labelpad - 5, 0), xycoords=ax.yaxis.label,\ntextcoords='offset points', ha='right', va='center', rotation=90, fontsize=11)\nif col_titles is not None:\nfor ax, column_title in zip(axs[0, :], col_titles):\nax.set_title(column_title)\nif plot_titles is not None:\nfor ax, subplot_title in zip(axs.flat, plot_titles):\nax.set_title(subplot_title)\n# default kwargs\nif 'plot_legend' not in kwargs:\nkwargs['plot_legend'] = False\n# loop through the grid of axes and plot the subsets\nif rows_by == cols_by:\nfor ax, row_val in zip(axs.flat, row_vals):\nsubset = ds.subset({rows_by: row_val})\ndataset_plot(subset, styler=styler, ax=ax, **kwargs)\nelse:\nfor row, row_val in enumerate(row_vals):\nfor col, col_val in enumerate(col_vals):\nax = axs[row, col]\nsubset = ds.subset({cols_by: col_val, rows_by: row_val})\nif len(subset) == 0: continue\ndataset_plot(subset, styler=styler, ax=ax, **kwargs)\nif subplot_cbar:\nplt.subplots_adjust(right=0.875)\ncax = plt.axes([0.9, 0.2, 0.014, 0.6])\ncbar = plt.colorbar(plt.cm.ScalarMappable(norm=styler.color_norm, cmap=styler.cmap), cax=cax)\ncbar.ax.yaxis.set_ticks_position('right')\ncbar.ax.yaxis.set_label_position('right')\nif styler.cbar_label is not None:\ncbar.set_label(styler.cbar_label)\nif subplot_legend:\nplt.subplots_adjust(right=0.835 + subplots_adjust)\naxs.flat[0].get_figure().legend(handles=styler.legend_handles(), loc='center right', frameon=False,\nbbox_to_anchor=(0.925, 0.5), markerfirst=True, handletextpad=0.05)\nreturn axs\n</code></pre>"},{"location":"reference/plotting/#paramaterial.plotting.info_plot","title":"<code>info_plot(ds, x, y, styler=None, ax=None, plot_legend=True, err_between=None, **kwargs)</code>","text":"<p>Make a single combined plot from the info of every dataitem in the dataset using pandas.DataFrame.plot.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>DataSet</code> <p>The dataset to plot.</p> required <code>x</code> <code>str</code> <p>The column to plot on the x-axis.</p> required <code>y</code> <code>str</code> <p>The column to plot on the y-axis.</p> required <code>styler</code> <code>Optional[Styler]</code> <p>The styler to use for the plot.</p> <code>None</code> <code>ax</code> <code>Optional[plt.Axes]</code> <p>The axis to plot on.</p> <code>None</code> <code>plot_legend</code> <code>bool</code> <p>Whether to plot the legend.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the pandas.DataFrame.plot function.</p> <code>{}</code> Source code in <code>paramaterial\\plotting.py</code> <pre><code>def info_plot(\nds: DataSet,\nx: str,\ny: str,\nstyler: Optional[Styler] = None,\nax: Optional[plt.Axes] = None,\nplot_legend: bool = True,\nerr_between: Optional[Tuple[str, str]] = None,\n**kwargs\n) -&gt; plt.Axes:\n\"\"\"Make a single combined plot from the info of every dataitem in the dataset using pandas.DataFrame.plot.\n    Args:\n        ds: The dataset to plot.\n        x: The column to plot on the x-axis.\n        y: The column to plot on the y-axis.\n        styler: The styler to use for the plot.\n        ax: The axis to plot on.\n        plot_legend: Whether to plot the legend.\n        **kwargs: Additional keyword arguments to pass to the pandas.DataFrame.plot function.\n    Returns: The axis the plot was made on.\n    \"\"\"\nif ax is None:\nfig, (ax) = plt.subplots(1, 1, figsize=kwargs.get('figsize', (6, 4)))\nkwargs['ax'] = ax\nif ax.get_legend() is not None and plot_legend:\nax.get_legend().remove()\nkwargs = {**styler.plot_kwargs, **kwargs}\n# plot the dataitems\nfor di in ds:\n# plot the curve\ndf = pd.DataFrame([[di.info[x], di.info[y]]], columns=[x, y])\nax = df.plot(x=x, y=y, **styler.curve_formatters(di), **kwargs)\n# ax = di.info.plot(x=x, y=y, **styler.curve_formatters(di), **kwargs)\nif err_between is not None:\nax = di.info.plot(x=x, y=y, yerr=[di.info[err_between[0]], di.info[err_between[1]]],\n**styler.curve_formatters(di), ax=ax)\n# add the legend\nhandles = styler.legend_handles(ds)\nif len(handles) &gt; 0 and plot_legend:\nax.legend(handles=handles, loc='best', frameon=True, markerfirst=False,\nhandletextpad=0.05)  # , labelspacing=0.1)\nax.get_legend().set_zorder(2000)\n# colorbar\nreturn ax\n</code></pre>"},{"location":"reference/plotting/#paramaterial.plotting.subplot_wrapper","title":"<code>subplot_wrapper(ds, plot_func, shape, rows_by, cols_by, row_vals, col_vals, axs=None, figsize=(12, 8), sharex='col', sharey='row', wspace=0.1, hspace=0.1, row_titles=None, col_titles=None, plot_titles=None, **kwargs)</code>","text":"<p>Plot a dataset using the given plot function as a grid of subplots, split by the 'rows_by' and 'cols_by' columns in the info_table.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>DataSet</code> <p>The dataset to plot.</p> required <code>plot_func</code> <code>Callable[[DataItem, plt.axes], DataItem]</code> <p>The function to use to plot each subplot.</p> required <code>shape</code> <code>Tuple[int, int]</code> <p>The shape of the grid of subplots.</p> required <code>rows_by</code> <code>str</code> <p>The column in the info_table to split the rows by.</p> required <code>cols_by</code> <code>str</code> <p>The column in the info_table to split the columns by.</p> required <code>row_vals</code> <code>List[List[Any]]</code> <p>The values of the rows to plot.</p> required <code>col_vals</code> <code>List[List[Any]]</code> <p>The values of the columns to plot.</p> required <code>axs</code> <code>Optional[np.ndarray]</code> <p>The axes to plot on.</p> <code>None</code> <code>figsize</code> <code>Tuple[float, float]</code> <p>The size of the figure.</p> <code>(12, 8)</code> <code>sharex</code> <code>str</code> <p>Whether to share the x axis between subplots.</p> <code>'col'</code> <code>sharey</code> <code>str</code> <p>Whether to share the y axis between subplots.</p> <code>'row'</code> <code>wspace</code> <code>float</code> <p>The width space between subplots.</p> <code>0.1</code> <code>hspace</code> <code>float</code> <p>The height space between subplots.</p> <code>0.1</code> <code>row_titles</code> <code>Optional[List[str]]</code> <p>The titles of the rows.</p> <code>None</code> <code>col_titles</code> <code>Optional[List[str]]</code> <p>The titles of the columns.</p> <code>None</code> <code>plot_titles</code> <code>Optional[List[str]]</code> <p>The titles of the subplots.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the plot function.</p> <code>{}</code> Source code in <code>paramaterial\\plotting.py</code> <pre><code>def subplot_wrapper(\nds: DataSet,\nplot_func: Callable[[DataItem, plt.axes], DataItem],\nshape: Tuple[int, int],\nrows_by: str,\ncols_by: str,\nrow_vals: List[List[Any]],\ncol_vals: List[List[Any]],\naxs: Optional[np.ndarray] = None,\nfigsize: Tuple[float, float] = (12, 8),\nsharex: str = 'col',\nsharey: str = 'row',\nwspace: float = 0.1,\nhspace: float = 0.1,\nrow_titles: Optional[List[str]] = None,\ncol_titles: Optional[List[str]] = None,\nplot_titles: Optional[List[str]] = None,\n**kwargs\n) -&gt; np.ndarray:\n\"\"\"Plot a dataset using the given plot function as a grid of subplots,\n    split by the 'rows_by' and 'cols_by' columns in the info_table.\n    Args:\n        ds: The dataset to plot.\n        plot_func: The function to use to plot each subplot.\n        shape: The shape of the grid of subplots.\n        rows_by: The column in the info_table to split the rows by.\n        cols_by: The column in the info_table to split the columns by.\n        row_vals: The values of the rows to plot.\n        col_vals: The values of the columns to plot.\n        axs: The axes to plot on.\n        figsize: The size of the figure.\n        sharex: Whether to share the x axis between subplots.\n        sharey: Whether to share the y axis between subplots.\n        wspace: The width space between subplots.\n        hspace: The height space between subplots.\n        row_titles: The titles of the rows.\n        col_titles: The titles of the columns.\n        plot_titles: The titles of the subplots.\n        **kwargs: Additional keyword arguments to pass to the plot function.\n    Returns: The axes the plot was made on.\n    \"\"\"\nif axs is None:\nfig, axs = plt.subplots(shape[0], shape[1], figsize=figsize, sharex=sharex, sharey=sharey)\nfig.subplots_adjust(wspace=wspace, hspace=hspace)\nif axs.ndim == 1:\naxs = np.array([axs])\n# loop through the grid of axes and plot the subsets\nif rows_by == cols_by:\nfor ax, row_val in zip(axs.flat, row_vals):\nkwargs['ax'] = ax\nsubset = ds.subset({rows_by: row_val})\nfor di in subset:\nplot_func(di, **kwargs)\nelse:\nfor row, row_val in enumerate(row_vals):\nfor col, col_val in enumerate(col_vals):\nkwargs['ax'] = axs[row, col]\nsubset = ds.subset({cols_by: col_val, rows_by: row_val})\nfor di in subset:\nplot_func(di, **kwargs)\n# add row titles\nif row_titles is not None:\nfor ax, row_title in zip(axs[:, 0], row_titles):\nax.annotate(row_title, xy=(0, 0.5), xytext=(-ax.yaxis.labelpad - 5, 0), xycoords=ax.yaxis.label,\ntextcoords='offset points', ha='right', va='center', rotation=90)\n# add column titles\nif col_titles is not None:\nfor ax, column_title in zip(axs[0, :], col_titles):\nax.set_title(column_title)\n# add subplot titles\nif plot_titles is not None:\nfor ax, subplot_title in zip(axs.flat, plot_titles):\nax.set_title(subplot_title)\nreturn axs\n</code></pre>"},{"location":"reference/plug/","title":"Plug","text":"<p>This module is responsible for handling data and executing I/O within the Paramaterial library.</p> The central components of the plug module are the DataSet and DataItem classes <ul> <li><code>DataItem</code>: A data structure that encapsulates a single test's information and data. A <code>DataItem</code> object holds:<ul> <li><code>test_id</code>: A string identifier for the test.</li> <li><code>data</code>: A pandas DataFrame containing the test data.</li> <li><code>info</code>: A pandas Series containing the metadata associated with the test.</li> </ul> </li> <li><code>DataSet</code>: A container class that manages a collection of <code>DataItem</code> objects. It provides various methods for managing and manipulating the data, including reading from files, writing to files, filtering, sorting, and applying custom functions.</li> </ul> Key Interactions between DataSet and DataItem <ul> <li>Loading Data: A <code>DataSet</code> is initialized by providing paths to metadata and data files. It reads the files and constructs a collection of <code>DataItem</code> objects.</li> <li>Accessing Data: You can access individual <code>DataItem</code> objects within a <code>DataSet</code> using index-based or test_id-based access through the <code>__getitem__</code> method.</li> <li>Applying Functions: You can use the <code>apply</code> method in the <code>DataSet</code> class to apply a custom function to each <code>DataItem</code>. This enables complex data transformations and analyses.</li> <li>Iterating: The <code>DataSet</code> class supports iteration over its <code>DataItem</code> objects, allowing you to loop through the data items using a standard <code>for</code> loop.</li> <li>Writing Data: The <code>write_output</code> method allows you to save the information in the <code>DataSet</code> back to files, preserving changes made to the <code>DataItem</code> objects.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Load a DataSet from files\n&gt;&gt;&gt; ds = DataSet('info.xlsx', 'data')\n&gt;&gt;&gt; # Access a specific DataItem by test_id\n&gt;&gt;&gt; di = ds['T01']\n&gt;&gt;&gt; # Apply a custom function to all DataItems\n&gt;&gt;&gt; def custom_function(di: DataItem) -&gt; DataItem:\n...     di.data['Stress_MPa'] *= 2\n...     di.info['max_stress'] = di.data['Stress_MPa'].max()\n...     return di\n&gt;&gt;&gt; ds_modified = ds.apply(custom_function)\n&gt;&gt;&gt; # Save the DataSet to new files\n&gt;&gt;&gt; ds_modified.write_output('new_info.xlsx', 'new_data')\n</code></pre>"},{"location":"reference/plug/#paramaterial.plug.DataItem","title":"<code>DataItem</code>  <code>dataclass</code>","text":"<p>A storage class for data and metadata related to a single test.</p> <p>Attributes:</p> Name Type Description <code>test_id</code> <code>str</code> <p>The unique identifier for the test.</p> <code>data</code> <code>pd.DataFrame</code> <p>A pandas DataFrame containing the data related to the test.</p> <code>info</code> <code>pd.Series</code> <p>A pandas Series containing metadata related to the test.</p> Source code in <code>paramaterial\\plug.py</code> <pre><code>@dataclass\nclass DataItem:\n\"\"\"A storage class for data and metadata related to a single test.\n    Attributes:\n        test_id: The unique identifier for the test.\n        data: A pandas DataFrame containing the data related to the test.\n        info: A pandas Series containing metadata related to the test.\n    \"\"\"\ntest_id: str\ndata: pd.DataFrame\ninfo: pd.Series\n</code></pre>"},{"location":"reference/plug/#paramaterial.plug.DataSet","title":"<code>DataSet</code>","text":"<p>A class for handling data, loading from files, and performing various operations.</p> <p>The DataSet class provides functionality for loading data from specified files, manipulating the data, and writing output. It contains a collection of DataItem objects, each representing a single test.</p> <p>Parameters:</p> Name Type Description Default <code>info_path</code> <code>Optional[str]</code> <p>The path to the info table file containing metadata.</p> <code>None</code> <code>data_dir</code> <code>Optional[str]</code> <p>The directory containing the data files.</p> <code>None</code> <code>test_id_key</code> <code>str</code> <p>The column name in the info table that contains the test IDs.</p> <code>'test_id'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = DataSet(info_path='info/01_prepared_info.xlsx', data_dir='data/01_prepared_data')\n&gt;&gt;&gt; len(ds)\n10\n</code></pre> <p>Raises:</p> Type Description <code>ValueError</code> <p>If only one of info_path and data_dir is specified.</p> <code>FileNotFoundError</code> <p>If a file is not found for a given test_id.</p> Source code in <code>paramaterial\\plug.py</code> <pre><code>class DataSet:\n\"\"\"A class for handling data, loading from files, and performing various operations.\n    The DataSet class provides functionality for loading data from specified files, manipulating the data,\n    and writing output. It contains a collection of DataItem objects, each representing a single test.\n    Args:\n        info_path: The path to the info table file containing metadata.\n        data_dir: The directory containing the data files.\n        test_id_key: The column name in the info table that contains the test IDs.\n    Examples:\n        &gt;&gt;&gt; ds = DataSet(info_path='info/01_prepared_info.xlsx', data_dir='data/01_prepared_data')\n        &gt;&gt;&gt; len(ds)\n        10\n    Raises:\n        ValueError: If only one of info_path and data_dir is specified.\n        FileNotFoundError: If a file is not found for a given test_id.\n    \"\"\"\ndef __init__(self, info_path: Optional[str] = None, data_dir: Optional[str] = None, test_id_key: str = 'test_id'):\nself.info_path = info_path\nself.data_dir = data_dir\nself.test_id_key = test_id_key\nif info_path is None and data_dir is None:\nself.data_items = []\nelif info_path is None or data_dir is None:\nraise ValueError('Both info_path and data_dir must be specified, or neither.')\nelse:\nself.data_items: List[DataItem] = self._load_data_items()\ndef _load_data_items(self) -&gt; List[DataItem]:\ninfo_table = _read_file(self.info_path)\ntry:\ntest_ids = info_table[self.test_id_key].tolist()\nexcept KeyError:\nraise KeyError(f'Could not find test_id column \"{self.test_id_key}\" in info_table.')\ndata_items = []\nfor test_id in test_ids:\nfile_path = os.path.join(self.data_dir, f'{test_id}.csv')\nif os.path.exists(file_path):\ninfo = info_table.loc[info_table[self.test_id_key] == test_id].iloc[0]\ndata = _read_file(file_path)\ndata_items.append(DataItem(test_id, data, info))\nelse:\nraise FileNotFoundError(f\"File not found for test_id={test_id}: {file_path}\")\nreturn data_items\n@property\ndef info_table(self) -&gt; pd.DataFrame:\ninfo_table = pd.DataFrame([di.info for di in self.data_items])\ninfo_table.index = range(len(info_table))\ninfo_table = info_table.apply(pd.to_numeric, errors='ignore')\nreturn info_table\n@info_table.setter\ndef info_table(self, info_table: pd.DataFrame):\n# Attempt to convert all columns to numeric, and ignore errors for non-numeric columns\ninfo_table = info_table.apply(pd.to_numeric, errors='ignore')\ndata_item_dict = {data_item.test_id: data_item for data_item in self.data_items}\ntest_ids = info_table[self.test_id_key].tolist()\nnew_data_items = []\nfor test_id in test_ids:\ndata_item = data_item_dict.get(test_id)\nif data_item:\n# Get the corresponding row from the info_table\nrow = info_table.loc[info_table[self.test_id_key] == test_id].iloc[0]\n# Update the data_item's info attribute\ndata_item.info = row\nnew_data_items.append(data_item)\nself.data_items = new_data_items\ndef write_output(self, info_path: str, data_dir: str) -&gt; None:\n\"\"\"Write the DataSet to files.\n        Args:\n            info_path: The path to the info table file to be written.\n            data_dir: The directory to write the data files to.\n        Examples:\n            &gt;&gt;&gt; ds.write_output(info_path='info/processed_info.xlsx', data_dir='data/processed_data')\n        Raises:\n            FileNotFoundError: If the data_dir does not exist.\n        \"\"\"\n# Create the data directory if it doesn't exist\nPath(data_dir).mkdir(parents=True, exist_ok=True)\n_write_file(self.info_table, info_path)\nfor di in self.data_items:\n_write_file(di.data, f'{data_dir}/{di.test_id}.csv')\ndef __iter__(self):\n\"\"\"Iterate over the DataItems in the DataSet.\n        Yields:\n            DataItem: The next DataItem in the DataSet.\n        Examples:\n            &gt;&gt;&gt; for di in ds:\n            ...     print(di.test_id)\n        \"\"\"\nfor test_id in tqdm(self.info_table[self.test_id_key].tolist(), unit='DataItems', leave=False):\ndata_item = next((di for di in self.data_items if di.test_id == test_id), None)\nif data_item is None:\nraise ValueError(f\"No DataItem found with test_id={test_id}.\")\nyield data_item\ndef apply(self, func: Callable[[DataItem, Dict], DataItem], **kwargs) -&gt; 'DataSet':\n\"\"\"Apply a function to each DataItem in the DataSet and return a new DataSet with the results.\n        Args:\n            func: The function to apply to each DataItem. It must take a DataItem and optional keyword arguments and\n            return a DataItem.\n            **kwargs: Additional keyword arguments to pass to the function.\n        Returns:\n            A new DataSet containing the DataItems after applying the function.\n        Examples:\n            &gt;&gt;&gt; def double_stress(di: DataItem) -&gt; DataItem:\n            ...     di.data['Stress_MPa'] *= 2\n            ...     return di\n            &gt;&gt;&gt; ds_doubled = ds.apply(double_stress)\n        \"\"\"\nnew_ds = self.copy()\nnew_ds.data_items = [func(di, **kwargs) for di in copy.deepcopy(self.data_items)]\nreturn new_ds\ndef copy(self) -&gt; 'DataSet':\n\"\"\"Create a copy of the DataSet.\n        Returns:\n            A copy of the DataSet.\n        Examples:\n            &gt;&gt;&gt; ds_copy = ds.copy()\n        \"\"\"\nnew_ds = DataSet(test_id_key=self.test_id_key)\nnew_ds.data_items = [DataItem(di.test_id, di.data.copy(), di.info.copy()) for di in self.data_items]\nreturn new_ds\ndef sort_by(self, column: Union[str, List[str]], ascending: bool = True) -&gt; 'DataSet':\n\"\"\"Sort a copy of the DataSet by a column in the info table and return the copy.\n        Args:\n            column: Column or list of columns to sort by.\n            ascending: Whether to sort in ascending order. (Default: True)\n        Returns:\n            A new DataSet sorted by the specified column(s).\n        Examples:\n            &gt;&gt;&gt; ds = DataSet(info_path='info/prepared_info.xlsx', data_dir='data/prepared_data')\n            &gt;&gt;&gt; ds_sorted = ds.sort_by('temperature')\n        \"\"\"\nnew_ds = self.copy()\nif isinstance(column, str):\ncolumn = [column]\n# Ensure that the specified columns are numeric before sorting\nfor col in column:\nif new_ds.info_table[col].dtype == 'object':\nnew_ds.info_table[col] = pd.to_numeric(new_ds.info_table[col], errors='ignore')\nnew_ds.info_table = new_ds.info_table.sort_values(by=column, ascending=ascending).reset_index(drop=True)\nreturn new_ds\ndef __getitem__(self, specifier: Union[int, str, slice]) -&gt; Union[List[DataItem], DataItem]:\n\"\"\"Get a DataItem or a list of DataItems by index, test_id, or slice.\n        Args:\n            specifier: An int for index-based access, a str for test_id-based access, or a slice for slicing.\n        Returns:\n            A DataItem or a list of DataItems depending on the specifier.\n        Examples:\n            &gt;&gt;&gt; di = ds[0]         # Get the first DataItem\n            &gt;&gt;&gt; di = ds['T01']     # Get the DataItem with test_id='T01'\n            &gt;&gt;&gt; dis = ds[0:5]      # Get the first five DataItems\n        Raises:\n            ValueError: If an invalid specifier type is provided.\n        \"\"\"\nif isinstance(specifier, int):\nreturn self.data_items[specifier]\nelif isinstance(specifier, str):\nreturn self.data_items[self.info_table[self.test_id_key].tolist().index(specifier)]\nelif isinstance(specifier, slice):\nreturn self.data_items[specifier]\nelse:\nraise ValueError(\nf'Invalid ds[&lt;specifier&gt;] specifier type: {type(specifier)}. Must be int, str (test_id), or slice.')\ndef subset(self, filter_dict: Dict[str, Union[str, List[Any]]]) -&gt; 'DataSet':\n\"\"\"Create a subset of the DataSet based on specified filters.\n        Args:\n            filter_dict: A dictionary containing column names as keys and values or list of values to filter by.\n        Returns:\n            A new DataSet containing only the filtered DataItems.\n        Examples:\n            &gt;&gt;&gt; ds_tensile = ds.subset({'test_type': ['T']})\n        Raises:\n            ValueError: If an invalid filter key is provided.\n        \"\"\"\nnew_ds = self.copy()\nfor key, value in filter_dict.items():\nif key not in new_ds.info_table.columns:\nraise ValueError(f'Invalid filter key: {key}. Must be one of {new_ds.info_table.columns}.')\nif not isinstance(value, list):\nfilter_dict[key] = [value]\nquery_string = ' and '.join([f\"`{key}` in {str(values)}\" for key, values in filter_dict.items()])\ntry:\nnew_info_table = new_ds.info_table.query(query_string)\nnew_ds.info_table = new_info_table\nexcept Exception as e:\nprint(f'Error applying query \"{query_string}\" to info_table: {e}')\nreturn new_ds\ndef __repr__(self):\nreturn f\"DataSet({len(self.data_items)} DataItems)\"\ndef __len__(self):\nreturn len(self.data_items)\n</code></pre>"},{"location":"reference/plug/#paramaterial.plug.DataSet.__getitem__","title":"<code>__getitem__(specifier)</code>","text":"<p>Get a DataItem or a list of DataItems by index, test_id, or slice.</p> <p>Parameters:</p> Name Type Description Default <code>specifier</code> <code>Union[int, str, slice]</code> <p>An int for index-based access, a str for test_id-based access, or a slice for slicing.</p> required <p>Returns:</p> Type Description <code>Union[List[DataItem], DataItem]</code> <p>A DataItem or a list of DataItems depending on the specifier.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; di = ds[0]         # Get the first DataItem\n&gt;&gt;&gt; di = ds['T01']     # Get the DataItem with test_id='T01'\n&gt;&gt;&gt; dis = ds[0:5]      # Get the first five DataItems\n</code></pre> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid specifier type is provided.</p> Source code in <code>paramaterial\\plug.py</code> <pre><code>def __getitem__(self, specifier: Union[int, str, slice]) -&gt; Union[List[DataItem], DataItem]:\n\"\"\"Get a DataItem or a list of DataItems by index, test_id, or slice.\n    Args:\n        specifier: An int for index-based access, a str for test_id-based access, or a slice for slicing.\n    Returns:\n        A DataItem or a list of DataItems depending on the specifier.\n    Examples:\n        &gt;&gt;&gt; di = ds[0]         # Get the first DataItem\n        &gt;&gt;&gt; di = ds['T01']     # Get the DataItem with test_id='T01'\n        &gt;&gt;&gt; dis = ds[0:5]      # Get the first five DataItems\n    Raises:\n        ValueError: If an invalid specifier type is provided.\n    \"\"\"\nif isinstance(specifier, int):\nreturn self.data_items[specifier]\nelif isinstance(specifier, str):\nreturn self.data_items[self.info_table[self.test_id_key].tolist().index(specifier)]\nelif isinstance(specifier, slice):\nreturn self.data_items[specifier]\nelse:\nraise ValueError(\nf'Invalid ds[&lt;specifier&gt;] specifier type: {type(specifier)}. Must be int, str (test_id), or slice.')\n</code></pre>"},{"location":"reference/plug/#paramaterial.plug.DataSet.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over the DataItems in the DataSet.</p> <p>Yields:</p> Name Type Description <code>DataItem</code> <p>The next DataItem in the DataSet.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; for di in ds:\n...     print(di.test_id)\n</code></pre> Source code in <code>paramaterial\\plug.py</code> <pre><code>def __iter__(self):\n\"\"\"Iterate over the DataItems in the DataSet.\n    Yields:\n        DataItem: The next DataItem in the DataSet.\n    Examples:\n        &gt;&gt;&gt; for di in ds:\n        ...     print(di.test_id)\n    \"\"\"\nfor test_id in tqdm(self.info_table[self.test_id_key].tolist(), unit='DataItems', leave=False):\ndata_item = next((di for di in self.data_items if di.test_id == test_id), None)\nif data_item is None:\nraise ValueError(f\"No DataItem found with test_id={test_id}.\")\nyield data_item\n</code></pre>"},{"location":"reference/plug/#paramaterial.plug.DataSet.apply","title":"<code>apply(func, **kwargs)</code>","text":"<p>Apply a function to each DataItem in the DataSet and return a new DataSet with the results.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[[DataItem, Dict], DataItem]</code> <p>The function to apply to each DataItem. It must take a DataItem and optional keyword arguments and</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataSet</code> <p>A new DataSet containing the DataItems after applying the function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def double_stress(di: DataItem) -&gt; DataItem:\n...     di.data['Stress_MPa'] *= 2\n...     return di\n&gt;&gt;&gt; ds_doubled = ds.apply(double_stress)\n</code></pre> Source code in <code>paramaterial\\plug.py</code> <pre><code>def apply(self, func: Callable[[DataItem, Dict], DataItem], **kwargs) -&gt; 'DataSet':\n\"\"\"Apply a function to each DataItem in the DataSet and return a new DataSet with the results.\n    Args:\n        func: The function to apply to each DataItem. It must take a DataItem and optional keyword arguments and\n        return a DataItem.\n        **kwargs: Additional keyword arguments to pass to the function.\n    Returns:\n        A new DataSet containing the DataItems after applying the function.\n    Examples:\n        &gt;&gt;&gt; def double_stress(di: DataItem) -&gt; DataItem:\n        ...     di.data['Stress_MPa'] *= 2\n        ...     return di\n        &gt;&gt;&gt; ds_doubled = ds.apply(double_stress)\n    \"\"\"\nnew_ds = self.copy()\nnew_ds.data_items = [func(di, **kwargs) for di in copy.deepcopy(self.data_items)]\nreturn new_ds\n</code></pre>"},{"location":"reference/plug/#paramaterial.plug.DataSet.copy","title":"<code>copy()</code>","text":"<p>Create a copy of the DataSet.</p> <p>Returns:</p> Type Description <code>DataSet</code> <p>A copy of the DataSet.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds_copy = ds.copy()\n</code></pre> Source code in <code>paramaterial\\plug.py</code> <pre><code>def copy(self) -&gt; 'DataSet':\n\"\"\"Create a copy of the DataSet.\n    Returns:\n        A copy of the DataSet.\n    Examples:\n        &gt;&gt;&gt; ds_copy = ds.copy()\n    \"\"\"\nnew_ds = DataSet(test_id_key=self.test_id_key)\nnew_ds.data_items = [DataItem(di.test_id, di.data.copy(), di.info.copy()) for di in self.data_items]\nreturn new_ds\n</code></pre>"},{"location":"reference/plug/#paramaterial.plug.DataSet.sort_by","title":"<code>sort_by(column, ascending=True)</code>","text":"<p>Sort a copy of the DataSet by a column in the info table and return the copy.</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>Union[str, List[str]]</code> <p>Column or list of columns to sort by.</p> required <code>ascending</code> <code>bool</code> <p>Whether to sort in ascending order. (Default: True)</p> <code>True</code> <p>Returns:</p> Type Description <code>DataSet</code> <p>A new DataSet sorted by the specified column(s).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds = DataSet(info_path='info/prepared_info.xlsx', data_dir='data/prepared_data')\n&gt;&gt;&gt; ds_sorted = ds.sort_by('temperature')\n</code></pre> Source code in <code>paramaterial\\plug.py</code> <pre><code>def sort_by(self, column: Union[str, List[str]], ascending: bool = True) -&gt; 'DataSet':\n\"\"\"Sort a copy of the DataSet by a column in the info table and return the copy.\n    Args:\n        column: Column or list of columns to sort by.\n        ascending: Whether to sort in ascending order. (Default: True)\n    Returns:\n        A new DataSet sorted by the specified column(s).\n    Examples:\n        &gt;&gt;&gt; ds = DataSet(info_path='info/prepared_info.xlsx', data_dir='data/prepared_data')\n        &gt;&gt;&gt; ds_sorted = ds.sort_by('temperature')\n    \"\"\"\nnew_ds = self.copy()\nif isinstance(column, str):\ncolumn = [column]\n# Ensure that the specified columns are numeric before sorting\nfor col in column:\nif new_ds.info_table[col].dtype == 'object':\nnew_ds.info_table[col] = pd.to_numeric(new_ds.info_table[col], errors='ignore')\nnew_ds.info_table = new_ds.info_table.sort_values(by=column, ascending=ascending).reset_index(drop=True)\nreturn new_ds\n</code></pre>"},{"location":"reference/plug/#paramaterial.plug.DataSet.subset","title":"<code>subset(filter_dict)</code>","text":"<p>Create a subset of the DataSet based on specified filters.</p> <p>Parameters:</p> Name Type Description Default <code>filter_dict</code> <code>Dict[str, Union[str, List[Any]]]</code> <p>A dictionary containing column names as keys and values or list of values to filter by.</p> required <p>Returns:</p> Type Description <code>DataSet</code> <p>A new DataSet containing only the filtered DataItems.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds_tensile = ds.subset({'test_type': ['T']})\n</code></pre> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid filter key is provided.</p> Source code in <code>paramaterial\\plug.py</code> <pre><code>def subset(self, filter_dict: Dict[str, Union[str, List[Any]]]) -&gt; 'DataSet':\n\"\"\"Create a subset of the DataSet based on specified filters.\n    Args:\n        filter_dict: A dictionary containing column names as keys and values or list of values to filter by.\n    Returns:\n        A new DataSet containing only the filtered DataItems.\n    Examples:\n        &gt;&gt;&gt; ds_tensile = ds.subset({'test_type': ['T']})\n    Raises:\n        ValueError: If an invalid filter key is provided.\n    \"\"\"\nnew_ds = self.copy()\nfor key, value in filter_dict.items():\nif key not in new_ds.info_table.columns:\nraise ValueError(f'Invalid filter key: {key}. Must be one of {new_ds.info_table.columns}.')\nif not isinstance(value, list):\nfilter_dict[key] = [value]\nquery_string = ' and '.join([f\"`{key}` in {str(values)}\" for key, values in filter_dict.items()])\ntry:\nnew_info_table = new_ds.info_table.query(query_string)\nnew_ds.info_table = new_info_table\nexcept Exception as e:\nprint(f'Error applying query \"{query_string}\" to info_table: {e}')\nreturn new_ds\n</code></pre>"},{"location":"reference/plug/#paramaterial.plug.DataSet.write_output","title":"<code>write_output(info_path, data_dir)</code>","text":"<p>Write the DataSet to files.</p> <p>Parameters:</p> Name Type Description Default <code>info_path</code> <code>str</code> <p>The path to the info table file to be written.</p> required <code>data_dir</code> <code>str</code> <p>The directory to write the data files to.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; ds.write_output(info_path='info/processed_info.xlsx', data_dir='data/processed_data')\n</code></pre> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the data_dir does not exist.</p> Source code in <code>paramaterial\\plug.py</code> <pre><code>def write_output(self, info_path: str, data_dir: str) -&gt; None:\n\"\"\"Write the DataSet to files.\n    Args:\n        info_path: The path to the info table file to be written.\n        data_dir: The directory to write the data files to.\n    Examples:\n        &gt;&gt;&gt; ds.write_output(info_path='info/processed_info.xlsx', data_dir='data/processed_data')\n    Raises:\n        FileNotFoundError: If the data_dir does not exist.\n    \"\"\"\n# Create the data directory if it doesn't exist\nPath(data_dir).mkdir(parents=True, exist_ok=True)\n_write_file(self.info_table, info_path)\nfor di in self.data_items:\n_write_file(di.data, f'{data_dir}/{di.test_id}.csv')\n</code></pre>"},{"location":"reference/preparing/","title":"Preparing","text":"<p>Functions to be used for preparing the experimental data for batch processing.</p>"},{"location":"reference/preparing/#paramaterial.preparing.check_column_headers","title":"<code>check_column_headers(data_dir, exception_headers=None)</code>","text":"<p>Check that all files in a directory have the same column headers and that column headers don't contain spaces. A ValueError will be raised if the column headers don't match or if a column header contains a space.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>Path to the directory containing the files to be checked.</p> required <code>exception_headers</code> <code>List[str]</code> <p>List of column headers that are allowed to be different between files.</p> <code>None</code> Source code in <code>paramaterial\\preparing.py</code> <pre><code>def check_column_headers(data_dir: str, exception_headers: List[str] = None):\n\"\"\"Check that all files in a directory have the same column headers and that column headers don't contain spaces.\n    A ValueError will be raised if the column headers don't match or if a column header contains a space.\n    Args:\n        data_dir: Path to the directory containing the files to be checked.\n        exception_headers: List of column headers that are allowed to be different between files.\n    \"\"\"\nfile_list = os.listdir(data_dir)\nfirst_file = pd.read_csv(f'{data_dir}/{file_list[0]}')\nfirst_columns = first_file.columns\nif exception_headers is not None:\nfor exception_header in exception_headers:\nif exception_header in first_columns:\nfirst_columns = first_columns.drop(exception_header)\nprint(\"Checking column headers...\")\nfor column_header in first_file.columns:\nif len(column_header.split(' ')) &gt; 1:\nraise ValueError(f'Column header \"{column_header}\" contains a space.')\nprint(f'First file headers:\\n\\t{list(first_file.columns)}')\nfor file in file_list[1:]:\ndf = pd.read_csv(f'{data_dir}/{file}')\ndf_columns = df.columns\nif exception_headers is not None:\nfor exception_header in exception_headers:\nif exception_header in df_columns:\ndf_columns = df_columns.drop(exception_header)\nif not df_columns.equals(first_columns):\nraise ValueError(f'Column headers in {file} don\\'t match column headers of first file.'\nf'{file} headers:\\n\\t{list(df.columns)}')\nprint(f'Headers in all files are the same as in the first file, except for {exception_headers}.')\n</code></pre>"},{"location":"reference/preparing/#paramaterial.preparing.check_for_duplicate_files","title":"<code>check_for_duplicate_files(data_dir)</code>","text":"<p>Check that there are no duplicate files in a directory by hashing the contents of the files. A ValueError will be raised if there are duplicate files.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>Path to the directory containing the files to be checked.</p> required Source code in <code>paramaterial\\preparing.py</code> <pre><code>def check_for_duplicate_files(data_dir: str):\n\"\"\"Check that there are no duplicate files in a directory by hashing the contents of the files.\n    A ValueError will be raised if there are duplicate files.\n    Args:\n        data_dir: Path to the directory containing the files to be checked.\n    \"\"\"\nprint('Checking for duplicate files...')\nhashes = [hash(open(f'{data_dir}/{file}', 'rb').read()) for file in os.listdir(data_dir)]\nif len(hashes) != len(set(hashes)):\nduplicates = [file for file, filehash in zip(os.listdir(data_dir), hashes) if hashes.count(filehash) &gt; 1]\nraise ValueError(f'There are duplicate files in {data_dir}.\\n'\n'The duplicates are:' + '\\n\\t'.join(duplicates))\nelse:\nprint(f'No duplicate files found in \"{data_dir}\".')\n</code></pre>"},{"location":"reference/preparing/#paramaterial.preparing.check_formatting","title":"<code>check_formatting(ds)</code>","text":"<p>Check that the formatting of the data is correct. This includes checking that the column headers are the same in all files, that the column headers don't contain spaces, and that there are no duplicate files. A ValueError will be raised if any of these conditions are not met.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>DataSet</code> <p>DataSet object containing the data to be checked.</p> required Source code in <code>paramaterial\\preparing.py</code> <pre><code>def check_formatting(ds: DataSet):\n\"\"\"Check that the formatting of the data is correct. This includes checking that the column headers are the same in all\n    files, that the column headers don't contain spaces, and that there are no duplicate files. A ValueError will be raised\n    if any of these conditions are not met.\n    Args:\n        ds: DataSet object containing the data to be checked.\n    \"\"\"\ncheck_column_headers(ds.data_dir)\ncheck_for_duplicate_files(ds.data_dir)\n</code></pre>"},{"location":"reference/preparing/#paramaterial.preparing.convert_gleeble_output_files_to_csv","title":"<code>convert_gleeble_output_files_to_csv(directory_path)</code>","text":"<p>Convert all files in a directory from Gleeble output format to csv format.</p> Source code in <code>paramaterial\\preparing.py</code> <pre><code>def convert_gleeble_output_files_to_csv(directory_path: str):\n\"\"\"Convert all files in a directory from Gleeble output format to csv format.\"\"\"\nfor file in os.listdir(directory_path):\nif not file.endswith('.csv'):\ndf = pd.read_csv(f'{directory_path}/{file}', header=[0, 1], delimiter='\\t')\ndf.columns = \\\n                [col[0] if str(col[1]).startswith('Unnamed') else ' '.join(col).strip() for col in df.columns]\ndf.to_csv(f'{directory_path}/{file[:-4]}.csv', index=False)\n</code></pre>"},{"location":"reference/preparing/#paramaterial.preparing.copy_data_and_rename_by_test_id","title":"<code>copy_data_and_rename_by_test_id(data_in, data_out, info_table, test_id_col='test_id')</code>","text":"<p>Rename files in data directory by test_id in info table and copy to new directory. The info_table must have a column named 'old_filename' containing the original filenames and a column named 'test_id'. The new filenames will be the test_ids with the extension '.csv'.</p> <p>Parameters:</p> Name Type Description Default <code>data_in</code> <code>str</code> <p>Path to the directory containing the data to be copied.</p> required <code>data_out</code> <code>str</code> <p>Path to the directory where the data will be copied.</p> required <code>info_table</code> <code>pd.DataFrame</code> <p>DataFrame containing the metadata for the tests.</p> required <code>test_id_col</code> <p>Column in the info table containing the test_ids.</p> <code>'test_id'</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>paramaterial\\preparing.py</code> <pre><code>def copy_data_and_rename_by_test_id(data_in: str, data_out: str, info_table: pd.DataFrame, test_id_col='test_id'):\n\"\"\"Rename files in data directory by test_id in info table and copy to new directory. The info_table must have a column\n    named 'old_filename' containing the original filenames and a column named 'test_id'. The new filenames will be the\n    test_ids with the extension '.csv'.\n    Args:\n        data_in: Path to the directory containing the data to be copied.\n        data_out: Path to the directory where the data will be copied.\n        info_table: DataFrame containing the metadata for the tests.\n        test_id_col: Column in the info table containing the test_ids.\n    Returns:\n        None\n    \"\"\"\n# make data directory if it doesn't exist\nif not os.path.exists(data_out):\nos.mkdir(data_out)\n# check info table\nif 'old_filename' not in info_table.columns:\nraise ValueError(f'There is no \"old_filename\" column in the info table.')\nif test_id_col not in info_table.columns:\nraise ValueError(f'There is no \"{test_id_col}\" column in the info table.')\nif info_table[test_id_col].duplicated().any():\nraise ValueError(f'There are duplicate test_ids.')\nif info_table['old_filename'].duplicated().any():\nraise ValueError(f'There are duplicate old_filenames.')\nfor filename, test_id in zip(info_table['old_filename'], info_table[test_id_col]):\n# check that file exists\nif not os.path.exists(f'{data_in}/{filename}'):\nraise FileNotFoundError(f'File {filename} does not exist in {data_in}.')\n# copy and rename file\nshutil.copy(f'{data_in}/{filename}', f'{data_out}/{test_id}.csv')\nprint(f'Copied {len(info_table)} files in {data_in} to {data_out}.')\n</code></pre>"},{"location":"reference/preparing/#paramaterial.preparing.experimental_matrix","title":"<code>experimental_matrix(info_table, index, columns, as_heatmap=False, title=None, xlabel=None, ylabel=None, tick_params=None, **kwargs)</code>","text":"<p>Make an experimental matrix showing the distribution of test across metadata categories.</p> <p>Parameters:</p> Name Type Description Default <code>info_table</code> <code>pd.DataFrame</code> <p>DataFrame containing the metadata for the tests.</p> required <code>index</code> <code>Union[str, List[str]]</code> <p>Column(s) of the info_table to use as the index of the matrix.</p> required <code>columns</code> <code>Union[str, List[str]]</code> <p>Column(s) of the info_table to use as the columns of the matrix.</p> required <code>as_heatmap</code> <code>bool</code> <p>If True, return a heatmap of the matrix. If False, return the matrix as a DataFrame.</p> <code>False</code> <code>title</code> <code>str</code> <p>Title of the heatmap.</p> <code>None</code> <code>xlabel</code> <code>str</code> <p>Label for the x-axis of the heatmap.</p> <code>None</code> <code>ylabel</code> <code>str</code> <p>Label for the y-axis of the heatmap.</p> <code>None</code> <code>tick_params</code> <code>Dict</code> <p>Parameters to pass to the ax.tick_params method of matplotlib.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the heatmap function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[pd.DataFrame, plt.Axes]</code> <p>If as_heatmap is False, returns a DataFrame of the experimental matrix.</p> <code>Union[pd.DataFrame, plt.Axes]</code> <p>If as_heatmap is True, returns a heatmap of the experimental matrix.</p> Source code in <code>paramaterial\\preparing.py</code> <pre><code>def experimental_matrix(info_table: pd.DataFrame, index: Union[str, List[str]], columns: Union[str, List[str]],\nas_heatmap: bool = False, title: str = None, xlabel: str = None,\nylabel: str = None, tick_params: Dict = None, **kwargs) -&gt; Union[pd.DataFrame, plt.Axes]:\n\"\"\"Make an experimental matrix showing the distribution of test across metadata categories.\n    Args:\n        info_table: DataFrame containing the metadata for the tests.\n        index: Column(s) of the info_table to use as the index of the matrix.\n        columns: Column(s) of the info_table to use as the columns of the matrix.\n        as_heatmap: If True, return a heatmap of the matrix. If False, return the matrix as a DataFrame.\n        title: Title of the heatmap.\n        xlabel: Label for the x-axis of the heatmap.\n        ylabel: Label for the y-axis of the heatmap.\n        tick_params: Parameters to pass to the ax.tick_params method of matplotlib.\n        **kwargs: Additional keyword arguments to pass to the heatmap function.\n    Returns:\n        If as_heatmap is False, returns a DataFrame of the experimental matrix.\n        If as_heatmap is True, returns a heatmap of the experimental matrix.\n    \"\"\"\n# make sure index and columns are lists\nif isinstance(index, str):\nindex = [index]\nif isinstance(columns, str):\ncolumns = [columns]\n# make the experimental matrix\nexp_matrix = info_table.groupby(index + columns).size().unstack(columns).fillna(0).astype(int)\n# return the matrix as a DataFrame if as_heatmap is False\nif not as_heatmap:\nreturn exp_matrix\n# update the default kwargs\ndefault_kwargs = dict(linewidths=2, cbar=False, annot=True, square=True, cmap='Blues')\ndefault_kwargs.update(kwargs)\nax = sns.heatmap(exp_matrix, **default_kwargs)\nif title:\nax.set_title(title)\nif xlabel:\nax.set_xlabel(xlabel)\nif ylabel:\nax.set_ylabel(ylabel)\nif tick_params:\nax.tick_params(**tick_params)\nreturn ax\n</code></pre>"},{"location":"reference/processing/","title":"Processing","text":"<p>Module with functions for processing materials test data. This includes functionality for finding properties like yield strength Young's modulus from stress-strain curves, as well as post-processing functions for cleaning and correcting experimental measurements.</p>"},{"location":"reference/processing/#paramaterial.processing.calculate_strain_rate","title":"<code>calculate_strain_rate(ds, strain_key='Strain', time_key='Time_s', strain_rate_key='Strain_Rate')</code>","text":"<p>Calculate the strain rate of a stress-strain curve.</p> <p>Parameters:</p> Name Type Description Default <code>di</code> <p>DataItem with stress-strain curve</p> required <code>strain_key</code> <code>str</code> <p>Key for strain data</p> <code>'Strain'</code> <code>time_key</code> <code>str</code> <p>Key for time data</p> <code>'Time_s'</code> <code>strain_rate_key</code> <code>str</code> <p>Key for strain rate data</p> <code>'Strain_Rate'</code> Source code in <code>paramaterial\\processing.py</code> <pre><code>def calculate_strain_rate(ds: DataSet, strain_key: str = 'Strain', time_key: str = 'Time_s',\nstrain_rate_key: str = 'Strain_Rate') -&gt; DataSet:\n\"\"\"Calculate the strain rate of a stress-strain curve.\n    Args:\n        di: DataItem with stress-strain curve\n        strain_key: Key for strain data\n        time_key: Key for time data\n        strain_rate_key: Key for strain rate data\n    Returns: DataItem with strain rate added to data.\n    \"\"\"\ndef _calculate_di_strain_rate(di):\ngradient = np.gradient(di.data[strain_key], di.data[time_key])\ndi.data[strain_rate_key] = gradient\ndi.data[f'Smoothed_{strain_rate_key}'] = np.convolve(gradient, np.ones(5)/60, mode='same')\nreturn di\nreturn ds.apply(_calculate_di_strain_rate)\n</code></pre>"},{"location":"reference/processing/#paramaterial.processing.correct_friction_UC","title":"<code>correct_friction_UC(di, mu_key='mu', h0_key='h_0', D0_key='D_0', disp_key='Disp(mm)', force_key='Force(kN)')</code>","text":"<p>Calculate the pressure and corrected stress for a uniaxial compression test with friction.</p> <p>Parameters:</p> Name Type Description Default <code>di</code> <code>DataItem</code> <p>DataItem with uniaxial compression test data</p> required <code>mu_key</code> <code>str</code> <p>Key for friction coefficient in info</p> <code>'mu'</code> <code>h0_key</code> <code>str</code> <p>Key for initial height in info</p> <code>'h_0'</code> <code>D0_key</code> <code>str</code> <p>Key for initial diameter in info</p> <code>'D_0'</code> <code>disp_key</code> <code>str</code> <p>Key for displacement data</p> <code>'Disp(mm)'</code> <code>force_key</code> <code>str</code> <p>Key for force data</p> <code>'Force(kN)'</code> Source code in <code>paramaterial\\processing.py</code> <pre><code>def correct_friction_UC(di: DataItem, mu_key: str = 'mu', h0_key: str = 'h_0', D0_key: str = 'D_0',\ndisp_key: str = 'Disp(mm)', force_key: str = 'Force(kN)') -&gt; DataItem:\n\"\"\"\n    Calculate the pressure and corrected stress for a uniaxial compression test with friction.\n    Args:\n        di: DataItem with uniaxial compression test data\n        mu_key: Key for friction coefficient in info\n        h0_key: Key for initial height in info\n        D0_key: Key for initial diameter in info\n        disp_key: Key for displacement data\n        force_key: Key for force data\n    Returns: DataItem with corrected stress and pressure added to data.\n    \"\"\"\nmu = di.info[mu_key]  # friction coefficient\nh_0 = di.info[h0_key]  # initial height in axial direction\nD_0 = di.info[D0_key]  # initial diameter\nh = h_0 - di.data[disp_key]  # instantaneous height\nd = D_0*np.sqrt(h_0/h)  # instantaneous diameter\nP = di.data[force_key]*1000*4/(np.pi*d**2)  # pressure (MPa)\ndi.data['Pressure(MPa)'] = P\ndi.data['Corrected_Stress(MPa)'] = P/(1 + (mu*d)/(3*h))  # correct stress\nreturn di\n</code></pre>"},{"location":"reference/processing/#paramaterial.processing.find_E","title":"<code>find_E(ds, LPL_stress, UPL_stress, strain_key='Strain', stress_key='Stress_MPa', E_key='E')</code>","text":"<p>Find the elastic modulus of a stress-strain curve by fitting a line to the points between the specified stresses.</p> <p>Parameters:</p> Name Type Description Default <code>di</code> <p>DataItem with stress-strain curve</p> required <code>LPL_stress</code> <code>float</code> <p>Lower stress bound</p> required <code>UPL_stress</code> <code>float</code> <p>Upper stress bound</p> required <code>strain_key</code> <code>str</code> <p>Key for strain data</p> <code>'Strain'</code> <code>stress_key</code> <code>str</code> <p>Key for stress data</p> <code>'Stress_MPa'</code> <code>E_key</code> <code>str</code> <p>Key to store elastic modulus in info</p> <code>'E'</code> Source code in <code>paramaterial\\processing.py</code> <pre><code>def find_E(ds: DataSet, LPL_stress: float, UPL_stress: float, strain_key: str = 'Strain',\nstress_key: str = 'Stress_MPa', E_key: str = 'E'):\n\"\"\"Find the elastic modulus of a stress-strain curve by fitting a line to the points between the specified stresses.\nArgs:\n        di: DataItem with stress-strain curve\n        LPL_stress: Lower stress bound\n        UPL_stress: Upper stress bound\n        strain_key: Key for strain data\n        stress_key: Key for stress data\n        E_key: Key to store elastic modulus in info\n    \"\"\"\nds = ds.copy()\ndef find_di_E(di):\nx_data = di.data[strain_key].values\ny_data = di.data[stress_key].values\nx = x_data[(y_data &gt;= LPL_stress) &amp; (y_data &lt;= UPL_stress)]\ny = y_data[(y_data &gt;= LPL_stress) &amp; (y_data &lt;= UPL_stress)]\nE = np.polyfit(x, y, 1)[0]\ndi.info[E_key] = E\nreturn di\nreturn ds.apply(find_di_E)\n</code></pre>"},{"location":"reference/processing/#paramaterial.processing.find_UTS","title":"<code>find_UTS(ds, strain_key='Strain', stress_key='Stress_MPa', max_strain=None)</code>","text":"<p>Find the ultimate tensile strength (UTS) of an engineering stress-strain curves in the DataSet. The UTS is defined as the maximum stress in the curve. The UTS is added to the DataSet.info dictionary as 'UTS_1' and the strain at the UTS is added as 'UTS_0'.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>DataSet</code> <p>DataSet containing the stress-strain curves.</p> required <code>strain_key</code> <code>str</code> <p>Key for the strain data in the DataItem.</p> <code>'Strain'</code> <code>stress_key</code> <code>str</code> <p>Key for the stress data in the DataItem.</p> <code>'Stress_MPa'</code> <code>max_strain</code> <code>Optional[float]</code> <p>Maximum strain to consider when finding the UTS. If None, the maximum strain in the curve is used.</p> <code>None</code> Source code in <code>paramaterial\\processing.py</code> <pre><code>def find_UTS(ds: DataSet, strain_key: str = 'Strain', stress_key: str = 'Stress_MPa',\nmax_strain: Optional[float] = None) -&gt; DataSet:\n\"\"\"Find the ultimate tensile strength (UTS) of an engineering stress-strain curves in the DataSet. The UTS is\n    defined\n    as the maximum stress in the curve. The UTS is added to the DataSet.info dictionary as 'UTS_1' and the strain at the\n    UTS is added as 'UTS_0'.\n    Args:\n        ds: DataSet containing the stress-strain curves.\n        strain_key: Key for the strain data in the DataItem.\n        stress_key: Key for the stress data in the DataItem.\n        max_strain: Maximum strain to consider when finding the UTS. If None, the maximum strain in the curve is used.\n    Returns: DataSet with UTS added to info_table.\n    \"\"\"\nds = ds.copy()\ndef find_di_UTS(di):\ndata = di.data[di.data[strain_key] &lt;= max_strain] if max_strain is not None else di.data\nx = data[strain_key].values\ny = data[stress_key].values\ndi.info['UTS_1'] = np.max(y)\ndi.info['UTS_0'] = x[np.argmax(y)]\nreturn di\nreturn ds.apply(find_di_UTS)\n</code></pre>"},{"location":"reference/processing/#paramaterial.processing.find_flow_stress_values","title":"<code>find_flow_stress_values(ds, strain_key='Strain', stress_key='Stress_MPa', temperature_key=None, rate_key=None, flow_strain=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>di</code> <p>DataItem with stress-strain curve</p> required <code>strain_key</code> <code>str</code> <p>Data key for reading strain.</p> <code>'Strain'</code> <code>stress_key</code> <code>str</code> <p>Data key for reading stress.</p> <code>'Stress_MPa'</code> <code>flow_strain_key</code> <p>Info key for writing flow strain.</p> required <code>flow_stress_key</code> <p>Info key for storing flow stress.</p> required <code>flow_strain</code> <code>Union[int, float, Tuple[float, float]]</code> <p>Strain at which to find the flow stress. If None, the maximum stress is used.</p> <code>None</code> Source code in <code>paramaterial\\processing.py</code> <pre><code>def find_flow_stress_values(ds: DataSet, strain_key: str = 'Strain', stress_key: str = 'Stress_MPa',\ntemperature_key: str = None, rate_key: str = None,\nflow_strain: Union[int, float, Tuple[float, float]] = None) -&gt; DataSet:\n\"\"\"\n    Args:\n        di: DataItem with stress-strain curve\n        strain_key: Data key for reading strain.\n        stress_key: Data key for reading stress.\n        flow_strain_key: Info key for writing flow strain.\n        flow_stress_key: Info key for storing flow stress.\n        flow_strain: Strain at which to find the flow stress. If None, the maximum stress is used.\n    Returns: DataItem with flow stress added to info.\n    \"\"\"\ndef find_di_flow_stress_values(di, flow_strain):\nif flow_strain is None:\nflow_strain = di.data[strain_key].max()\nif (type(flow_strain) is float) or (type(flow_strain) is int):\ndi.info[f'flow_{strain_key}'] = flow_strain\ndi.info[f'flow_{stress_key}'] = di.data[stress_key][di.data[strain_key] &lt;= flow_strain].max()\nif temperature_key is not None:\ndi.info[f'flow_{temperature_key}'] = di.data[temperature_key][di.data[strain_key] &lt;= flow_strain].max()\nif rate_key is not None:\ndi.info[f'flow_{rate_key}'] = di.data[rate_key][di.data[strain_key] &lt;= flow_strain].max()\nelif type(flow_strain) is tuple:\n# average the flow stress over a range of strains\ndi.info[f'flow_{strain_key}'] = np.mean(flow_strain)\ndi.info[f'flow_{stress_key}'] = di.data[stress_key][\n(di.data[strain_key] &gt;= flow_strain[0])&amp;(di.data[strain_key] &lt;= flow_strain[1])].mean()\nif temperature_key is not None:\ndi.info[f'flow_{temperature_key}'] = di.data[temperature_key][\n(di.data[strain_key] &gt;= flow_strain[0])&amp;(di.data[strain_key] &lt;= flow_strain[1])].mean()\nif rate_key is not None:\ndi.info[f'flow_{rate_key}'] = di.data[rate_key][\n(di.data[strain_key] &gt;= flow_strain[0])&amp;(di.data[strain_key] &lt;= flow_strain[1])].mean()\nreturn di\nreturn ds.apply(find_di_flow_stress_values, flow_strain=flow_strain)\n</code></pre>"},{"location":"reference/processing/#paramaterial.processing.find_fracture_point","title":"<code>find_fracture_point(ds, strain_key='Strain', stress_key='Stress_MPa')</code>","text":"<p>Find the fracture point for the stress-strain curves in the DataSet. The fracture point is defined as the maximum strain in the curve. The fracture point is added to the DataSet.info dictionary as 'FP_1' and the stress at the fracture point is added as 'FP_0'.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>DataSet</code> <p>DataSet with stress-strain curves</p> required <code>strain_key</code> <code>str</code> <p>Key for strain data</p> <code>'Strain'</code> <code>stress_key</code> <code>str</code> <p>Key for stress data</p> <code>'Stress_MPa'</code> Source code in <code>paramaterial\\processing.py</code> <pre><code>def find_fracture_point(ds: DataSet, strain_key: str = 'Strain', stress_key: str = 'Stress_MPa') -&gt; DataSet:\n\"\"\"Find the fracture point for the stress-strain curves in the DataSet. The fracture point is defined as the\n    maximum strain in the curve. The fracture point is added to the DataSet.info dictionary as 'FP_1' and the stress at\n    the fracture point is added as 'FP_0'.\n    Args:\n        ds: DataSet with stress-strain curves\n        strain_key: Key for strain data\n        stress_key: Key for stress data\n    Returns: DataSet with fracture point added to info_table.\n    \"\"\"\nds = ds.copy()\ndef find_di_fracture_point(di):\nidx_max = di.data[strain_key].idxmax()\ndi.info['FP_0'] = di.data[strain_key][idx_max]\ndi.info['FP_1'] = di.data[stress_key][idx_max]\nreturn di\nreturn ds.apply(find_di_fracture_point)\n</code></pre>"},{"location":"reference/processing/#paramaterial.processing.find_proof_stress","title":"<code>find_proof_stress(ds, proof_strain=0.002, strain_key='Strain', stress_key='Stress_MPa', E_key='E')</code>","text":"<p>Find the proof stress of a stress-strain curve.</p> <p>Parameters:</p> Name Type Description Default <code>di</code> <p>DataItem with stress-strain curve</p> required <code>proof_strain</code> <code>float</code> <p>Strain at which to find the proof stress</p> <code>0.002</code> <code>strain_key</code> <code>str</code> <p>Key for strain data</p> <code>'Strain'</code> <code>stress_key</code> <code>str</code> <p>Key for stress data</p> <code>'Stress_MPa'</code> Source code in <code>paramaterial\\processing.py</code> <pre><code>def find_proof_stress(ds: DataSet, proof_strain: float = 0.002, strain_key: str = 'Strain',\nstress_key: str = 'Stress_MPa', E_key: str = 'E') -&gt; DataSet:\n\"\"\"Find the proof stress of a stress-strain curve.\n    Args:\n        di: DataItem with stress-strain curve\n        proof_strain: Strain at which to find the proof stress\n        strain_key: Key for strain data\n        stress_key: Key for stress data\n    Returns: DataItem with proof stress added to info.\n    \"\"\"\ndef find_di_proof_stress(di):\nE = di.info[E_key]\nx_data = di.data[strain_key].values\ny_data = di.data[stress_key].values\nx_shift = proof_strain\ny_line = E*(x_data - x_shift)\ntry:\ncut = np.where(np.diff(np.sign(y_line - y_data)) != 0)[0][-1]\nm = (y_data[cut + 1] - y_data[cut])/(x_data[cut + 1] - x_data[cut])\nxl = x_data[cut]\nyl = y_line[cut]\nxd = x_data[cut]\nyd = y_data[cut]\nK = np.array([[1, -E], [1, -m]])\nf = np.array([[yl - E*xl], [yd - m*xd]])\nd = np.linalg.solve(K, f).flatten()\ndi.info[f'PS_{proof_strain}_0'] = d[1]\ndi.info[f'PS_{proof_strain}_1'] = d[0]\nexcept IndexError:\ndi.info[f'PS_{proof_strain}_0'] = np.nan\ndi.info[f'PS_{proof_strain}_1'] = np.nan\nreturn di\nreturn ds.apply(find_di_proof_stress)\n</code></pre>"},{"location":"reference/processing/#paramaterial.processing.find_upl_and_lpl","title":"<code>find_upl_and_lpl(ds, strain_key='Strain', stress_key='Stress_MPa', preload=0, preload_key='Stress_MPa', max_strain=None, suppress_numpy_warnings=True)</code>","text":"<p>Determine the upper proportional limit (UPL) and lower proportional limit (LPL) of a stress-strain curve. The UPL is the point that minimizes the residuals of the slope fit between that point and the specified preload. The LPL is the point that minimizes the residuals of the slope fit between that point and the UPL. The elastic modulus is the slope between the UPL and LPL.</p> <p>Parameters:</p> Name Type Description Default <code>di</code> <p>DataItem with stress-strain curve</p> required <code>strain_key</code> <code>str</code> <p>key for strain data</p> <code>'Strain'</code> <code>stress_key</code> <code>str</code> <p>key for stress data</p> <code>'Stress_MPa'</code> <code>preload</code> <code>float</code> <p>preload value</p> <code>0</code> <code>preload_key</code> <code>str</code> <p>key for preload data</p> <code>'Stress_MPa'</code> <code>max_strain</code> <code>Optional[float]</code> <p>maximum strain to consider</p> <code>None</code> <code>suppress_numpy_warnings</code> <code>bool</code> <p>suppress numpy warnings</p> <code>True</code> <p>Returns:</p> Type Description <code>DataSet</code> <p>DataItem with UPL, LPL, and E added to info.</p> Source code in <code>paramaterial\\processing.py</code> <pre><code>def find_upl_and_lpl(ds: DataSet, strain_key: str = 'Strain', stress_key: str = 'Stress_MPa', preload: float = 0,\npreload_key: str = 'Stress_MPa', max_strain: Optional[float] = None,\nsuppress_numpy_warnings: bool = True) -&gt; DataSet:\n\"\"\"Determine the upper proportional limit (UPL) and lower proportional limit (LPL) of a stress-strain curve.\n    The UPL is the point that minimizes the residuals of the slope fit between that point and the specified preload.\n    The LPL is the point that minimizes the residuals of the slope fit between that point and the UPL.\n    The elastic modulus is the slope between the UPL and LPL.\n    Args:\n        di: DataItem with stress-strain curve\n        strain_key: key for strain data\n        stress_key: key for stress data\n        preload: preload value\n        preload_key: key for preload data\n        max_strain: maximum strain to consider\n        suppress_numpy_warnings: suppress numpy warnings\n    Returns:\n        DataItem with UPL, LPL, and E added to info.\n    \"\"\"\nif suppress_numpy_warnings:\nnp.seterr(all=\"ignore\")\nds = ds.copy()\ndef _find_upl_and_lpl(di: DataItem) -&gt; DataItem:\ndata = di.data[di.data[strain_key] &lt;= max_strain] if max_strain is not None else di.data\nUPL = (0, 0)\nLPL = (0, 0)\ndef fit_line(_x, _y):\nn = len(_x)  # number of points\nm = (n*np.sum(_x*_y) - np.sum(_x)*np.sum(_y))/(n*np.sum(np.square(_x)) - np.square(np.sum(_x)))  # slope\nc = (np.sum(_y) - m*np.sum(_x))/n  # intercept\nS_xy = (n*np.sum(_x*_y) - np.sum(_x)*np.sum(_y))/(n - 1)  # empirical covariance\nS_x = np.sqrt((n*np.sum(np.square(_x)) - np.square(np.sum(_x)))/(n - 1))  # x standard deviation\nS_y = np.sqrt((n*np.sum(np.square(_y)) - np.square(np.sum(_y)))/(n - 1))  # y standard deviation\nr = S_xy/(S_x*S_y)  # correlation coefficient\nS_m = np.sqrt((1 - r**2)/(n - 2))*S_y/S_x  # slope standard deviation\nS_rel = S_m/m  # relative deviation of slope\nreturn S_rel\nx = data[strain_key].values\ny = data[stress_key].values\nx_upl = x[data[preload_key] &gt;= preload]\ny_upl = y[data[preload_key] &gt;= preload]\nS_min = np.inf\nfor i in range(3, len(x_upl)):\nS_rel = fit_line(x_upl[:i], y_upl[:i])  # fit a line to the first i points after the preload\nif S_rel &lt; S_min:\nS_min = S_rel\nUPL = (x_upl[i], y_upl[i])\nx_lpl = x[x &lt;= UPL[0]]\ny_lpl = y[x &lt;= UPL[0]]\nS_min = np.inf\nfor j in range(len(x), 3, -1):\nS_rel = fit_line(x_lpl[j:], y_lpl[j:])  # fit a line to the last i points before the UPL\nif S_rel &lt; S_min:\nS_min = S_rel\nLPL = [x_lpl[j], y_lpl[j]]\n# if LPL is less than preload, then find the first data point with stress greater than preload\nif LPL[1] &lt; preload:\nLPL = (x[data[preload_key] &gt;= preload][0], y[data[preload_key] &gt;= preload][0])\ndi.info['UPL_0'] = UPL[0]\ndi.info['UPL_1'] = UPL[1]\ndi.info['LPL_0'] = LPL[0]\ndi.info['LPL_1'] = LPL[1]\ndi.info['E'] = (UPL[1] - LPL[1])/(UPL[0] - LPL[0])\nreturn di\nreturn ds.apply(_find_upl_and_lpl)\n</code></pre>"},{"location":"reference/receipts/","title":"Receipts","text":"<p>Module containing the class for generating test receipts.</p>"},{"location":"reference/receipts/#paramaterial.receipts.TestReceipts","title":"<code>TestReceipts</code>","text":"<p>Class for generating test receipts.</p> <p>Parameters:</p> Name Type Description Default <code>template_path</code> <code>str</code> <p>Path to the template file to be used for generating the receipts.</p> required <code>jinja_env_kwargs</code> <code>Dict</code> <p>Keyword arguments to be passed to the jinja2.Environment constructor.</p> <code>None</code> Source code in <code>paramaterial\\receipts.py</code> <pre><code>class TestReceipts:\n\"\"\"Class for generating test receipts.\n    Args:\n        template_path: Path to the template file to be used for generating the receipts.\n        jinja_env_kwargs: Keyword arguments to be passed to the jinja2.Environment constructor.\n    \"\"\"\ndef __init__(self, template_path: str, jinja_env_kwargs: Dict = None):\nself.template_path = template_path\nself.placeholders: List[str] = ['']\ndefault_jinja_env_kwargs = dict(variable_start_string=r'\\VAR{',\nvariable_end_string='}',\nautoescape=False,\nloader=FileSystemLoader(os.path.abspath('.')),\ncomment_start_string='%',\ncomment_end_string='#')\nif jinja_env_kwargs is None:\nenv_kwargs = default_jinja_env_kwargs\nelse:\nenv_kwargs = default_jinja_env_kwargs.update(jinja_env_kwargs)\nself.jinja_env = Environment(**env_kwargs)\ndef parse_placeholders(self, as_dict: bool = False) -&gt; Union[List[str], Dict[str, Any]]:\n\"\"\"Parse the template file for placeholders. With the default setup for the jinja2.Environment, placeholders are\n        defined as \\VAR{placeholder_name}. The placeholders are returned as a list of strings, or as a dictionary with\n        the placeholder names as keys and None as values.\n        Args:\n            as_dict: If True, return a dictionary with the placeholder names as keys and None as values. If False,\n            return a list of strings.\n        \"\"\"\ntemplate_source = self.jinja_env.loader.get_source(self.jinja_env, self.template_path)\nparsed_content = self.jinja_env.parse(template_source)\nself.placeholders = list(meta.find_undeclared_variables(parsed_content))\nif as_dict:\nreturn {key: None for key in self.placeholders}\nreturn self.placeholders\ndef generate_receipts(self, ds: DataSet, receipts_path: str, replace_dict: Dict[str, Any],\nreceipts_dir: str = './receipts', clean: bool = True):\n\"\"\"Generate receipts for the tests in the DataSet. The receipts are saved as pdf files in the receipts_dir\n        directory. The directory structure is receipts_dir/test_id/(files for test_id receipt). The receipts are merged\n        into a single pdf file and saved at receipts_path. The replace_dict dictionary is used to replace the\n        placeholders\n        in the template file. The keys of the dictionary are the placeholders, and the values are the replacement\n        strings.\n        The values can also be functions that take a DataItem as input and return a string. The functions are called\n        with the DataItem corresponding to the test_id of the receipt being generated. If the function generates a\n        plot, it should save the plot in the current directory and return the name of the saved plot-file\"\"\"\n# make receipts folder\nif not os.path.exists(receipts_dir):\nos.mkdir(receipts_dir)\npage_num = 1\nfor di in ds.data_items:\n# make di folder\ndi_folder = os.path.join(receipts_dir, di.test_id)\nif not os.path.exists(di_folder):\nos.mkdir(di_folder)\n# change to di folder\nsrc_wd = os.getcwd()\n# try make receipt, if error, go back to src_wd\ntry:\nos.chdir(f'{receipts_dir}/{di.test_id}')\n# call replacer functions and fill template\ntemplate = self.jinja_env.get_template(self.template_path)\nreplace_dict_strings = {}\nfor placeholder, replacer in replace_dict.items():\nif callable(replacer):\nreplace_dict_strings.update({placeholder: replacer(di)})\nplt.close()\nelse:\nreplace_dict_strings.update({placeholder: replacer})\nfilled_template = template.render(**replace_dict_strings).replace('_', '\\_')\n# set page number\nfilled_template = filled_template.replace('\\end{document}',\nf'\\setcounter{{page}}{{{page_num}}}\\n\\end{{document}}')\npage_num += 1\n# write filled template to file\nwith open(f'{di.test_id}_receipt.tex', 'w') as f:\nf.write(filled_template)\n# compile pdf\ncmd = ['pdflatex', '-interaction', 'nonstopmode', f'{di.test_id}_receipt.tex']\nproc = subprocess.Popen(cmd)\nproc.communicate()\nretcode = proc.returncode\nif not retcode == 0:\nos.unlink(f'{di.test_id}_receipt.pdf')\nraise ValueError('Error {} executing command: {}'.format(retcode, ' '.join(cmd)))\nexcept Exception as e:\nprint(e)\nfinally:\nos.chdir(src_wd)\n# change back to receipts folder\nos.chdir(src_wd)\n# merge pdfs\npdfs = [os.path.join(receipts_dir, di.test_id, f'{di.test_id}_receipt.pdf') for di in ds.data_items]\nmerger = PdfMerger()\nfor pdf in pdfs:\nmerger.append(pdf)\nmerger.write(receipts_path)\nmerger.close()\n# delete receipts folder\nif clean:\nshutil.rmtree(receipts_dir)\n</code></pre>"},{"location":"reference/receipts/#paramaterial.receipts.TestReceipts.generate_receipts","title":"<code>generate_receipts(ds, receipts_path, replace_dict, receipts_dir='./receipts', clean=True)</code>","text":"<p>Generate receipts for the tests in the DataSet. The receipts are saved as pdf files in the receipts_dir directory. The directory structure is receipts_dir/test_id/(files for test_id receipt). The receipts are merged into a single pdf file and saved at receipts_path. The replace_dict dictionary is used to replace the placeholders in the template file. The keys of the dictionary are the placeholders, and the values are the replacement strings. The values can also be functions that take a DataItem as input and return a string. The functions are called with the DataItem corresponding to the test_id of the receipt being generated. If the function generates a plot, it should save the plot in the current directory and return the name of the saved plot-file</p> Source code in <code>paramaterial\\receipts.py</code> <pre><code>def generate_receipts(self, ds: DataSet, receipts_path: str, replace_dict: Dict[str, Any],\nreceipts_dir: str = './receipts', clean: bool = True):\n\"\"\"Generate receipts for the tests in the DataSet. The receipts are saved as pdf files in the receipts_dir\n    directory. The directory structure is receipts_dir/test_id/(files for test_id receipt). The receipts are merged\n    into a single pdf file and saved at receipts_path. The replace_dict dictionary is used to replace the\n    placeholders\n    in the template file. The keys of the dictionary are the placeholders, and the values are the replacement\n    strings.\n    The values can also be functions that take a DataItem as input and return a string. The functions are called\n    with the DataItem corresponding to the test_id of the receipt being generated. If the function generates a\n    plot, it should save the plot in the current directory and return the name of the saved plot-file\"\"\"\n# make receipts folder\nif not os.path.exists(receipts_dir):\nos.mkdir(receipts_dir)\npage_num = 1\nfor di in ds.data_items:\n# make di folder\ndi_folder = os.path.join(receipts_dir, di.test_id)\nif not os.path.exists(di_folder):\nos.mkdir(di_folder)\n# change to di folder\nsrc_wd = os.getcwd()\n# try make receipt, if error, go back to src_wd\ntry:\nos.chdir(f'{receipts_dir}/{di.test_id}')\n# call replacer functions and fill template\ntemplate = self.jinja_env.get_template(self.template_path)\nreplace_dict_strings = {}\nfor placeholder, replacer in replace_dict.items():\nif callable(replacer):\nreplace_dict_strings.update({placeholder: replacer(di)})\nplt.close()\nelse:\nreplace_dict_strings.update({placeholder: replacer})\nfilled_template = template.render(**replace_dict_strings).replace('_', '\\_')\n# set page number\nfilled_template = filled_template.replace('\\end{document}',\nf'\\setcounter{{page}}{{{page_num}}}\\n\\end{{document}}')\npage_num += 1\n# write filled template to file\nwith open(f'{di.test_id}_receipt.tex', 'w') as f:\nf.write(filled_template)\n# compile pdf\ncmd = ['pdflatex', '-interaction', 'nonstopmode', f'{di.test_id}_receipt.tex']\nproc = subprocess.Popen(cmd)\nproc.communicate()\nretcode = proc.returncode\nif not retcode == 0:\nos.unlink(f'{di.test_id}_receipt.pdf')\nraise ValueError('Error {} executing command: {}'.format(retcode, ' '.join(cmd)))\nexcept Exception as e:\nprint(e)\nfinally:\nos.chdir(src_wd)\n# change back to receipts folder\nos.chdir(src_wd)\n# merge pdfs\npdfs = [os.path.join(receipts_dir, di.test_id, f'{di.test_id}_receipt.pdf') for di in ds.data_items]\nmerger = PdfMerger()\nfor pdf in pdfs:\nmerger.append(pdf)\nmerger.write(receipts_path)\nmerger.close()\n# delete receipts folder\nif clean:\nshutil.rmtree(receipts_dir)\n</code></pre>"},{"location":"reference/receipts/#paramaterial.receipts.TestReceipts.parse_placeholders","title":"<code>parse_placeholders(as_dict=False)</code>","text":"<p>Parse the template file for placeholders. With the default setup for the jinja2.Environment, placeholders are defined as \\VAR{placeholder_name}. The placeholders are returned as a list of strings, or as a dictionary with the placeholder names as keys and None as values.</p> <p>Parameters:</p> Name Type Description Default <code>as_dict</code> <code>bool</code> <p>If True, return a dictionary with the placeholder names as keys and None as values. If False,</p> <code>False</code> Source code in <code>paramaterial\\receipts.py</code> <pre><code>def parse_placeholders(self, as_dict: bool = False) -&gt; Union[List[str], Dict[str, Any]]:\n\"\"\"Parse the template file for placeholders. With the default setup for the jinja2.Environment, placeholders are\n    defined as \\VAR{placeholder_name}. The placeholders are returned as a list of strings, or as a dictionary with\n    the placeholder names as keys and None as values.\n    Args:\n        as_dict: If True, return a dictionary with the placeholder names as keys and None as values. If False,\n        return a list of strings.\n    \"\"\"\ntemplate_source = self.jinja_env.loader.get_source(self.jinja_env, self.template_path)\nparsed_content = self.jinja_env.parse(template_source)\nself.placeholders = list(meta.find_undeclared_variables(parsed_content))\nif as_dict:\nreturn {key: None for key in self.placeholders}\nreturn self.placeholders\n</code></pre>"},{"location":"reference/screening/","title":"Screening","text":"<p>Module with functions for generating and reading a screening pdf file.</p>"},{"location":"reference/screening/#paramaterial.screening.make_screening_pdf","title":"<code>make_screening_pdf(ds, plot_func, pdf_path='screening_pdf.pdf', pagesize=(900, 600))</code>","text":"<p>Make a screening pdf where each page contains a plot, a check-box and a comment-box. Scaling the plot is still under development. Currently, the plot_func should produce a figure with figsize=(10., 5.8).</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>DataSet</code> <p>DataSet object</p> required <code>plot_func</code> <code>Callable[[DataItem], None]</code> <p>Function that takes a DataItem and generates a plot</p> required <code>pdf_path</code> <code>str</code> <p>Path where pdf will be saved</p> <code>'screening_pdf.pdf'</code> <code>pagesize</code> <code>Tuple[float, float]</code> <p>Size of the pdf pages</p> <code>(900, 600)</code> Source code in <code>paramaterial\\screening.py</code> <pre><code>def make_screening_pdf(\nds: DataSet,\nplot_func: Callable[[DataItem], None],\npdf_path: str = 'screening_pdf.pdf',\npagesize: Tuple[float, float] = (900, 600),\n) -&gt; None:\n\"\"\"Make a screening pdf where each page contains a plot, a check-box and a comment-box.\n    Scaling the plot is still under development. Currently, the plot_func should produce a figure with figsize=(10., 5.8).\n    Args:\n        ds: DataSet object\n        plot_func: Function that takes a DataItem and generates a plot\n        pdf_path: Path where pdf will be saved\n        pagesize: Size of the pdf pages\n    Returns: None\n    \"\"\"\n# setup canvas\npdf_canvas = canvas.Canvas(pdf_path, pagesize=(pagesize[0], pagesize[1]))\n# loop through dataitems\nfor di in ds:\n# make plot for dataitem\nplot_func(di)\n# add plot to page\nimgdata = BytesIO()\nplt.savefig(imgdata, format='svg')\nimgdata.seek(0)\ndrawing = svg2rlg(imgdata)\nrenderPDF.draw(drawing, pdf_canvas, 0.001*pagesize[0], 0.15*pagesize[1])\n# setup form\nform = pdf_canvas.acroForm\npdf_canvas.setFont(\"Courier\", plt.rcParams['font.size'] + 6)\n# add test_id\npdf_canvas.drawString(0.05*pagesize[0], 0.95*pagesize[1], f'{di.test_id}')\n# add checkbox\npdf_canvas.drawString(0.05*pagesize[0], 0.14*pagesize[1], 'REJECT?:')\nform.checkbox(name=f'reject_box_{di.test_id}', buttonStyle='check',\nx=0.15*pagesize[0], y=0.13*pagesize[1],\nborderColor=magenta, fillColor=pink, textColor=blue, forceBorder=True)\n# add text field\npdf_canvas.drawString(0.05*pagesize[0], 0.08*pagesize[1], 'COMMENT:')\nform.textfield(name=f'comment_box_{di.test_id}', maxlen=10000,\nx=0.15*pagesize[0], y=0.05*pagesize[1], width=0.7*pagesize[0], height=0.05*pagesize[1],\nborderColor=magenta, fillColor=pink, textColor=black, forceBorder=True, fieldFlags='multiline')\n# add page to canvas and close plot\npdf_canvas.showPage()\nplt.close()\npdf_canvas.save()\nprint(f'Screening pdf saved to {pdf_path}.')\n</code></pre>"},{"location":"reference/screening/#paramaterial.screening.read_screening_pdf","title":"<code>read_screening_pdf(ds, pdf_path)</code>","text":"<p>Read the values from the checkbox and comment fields in the screening pdf and add them to the DataSet's info_table. The info_table will have two new columns: 'reject' and 'comment'. The 'reject' column will contain either 'True' or 'False'. The 'comment' column will contain the comment string entered into the comment field.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>DataSet</code> <p>DataSet object</p> required <code>pdf_path</code> <code>str</code> <p>Path to screening pdf file</p> required Source code in <code>paramaterial\\screening.py</code> <pre><code>def read_screening_pdf(ds: DataSet, pdf_path: str) -&gt; DataSet:\n\"\"\"Read the values from the checkbox and comment fields in the screening pdf and add them to the DataSet's info_table.\n    The info_table will have two new columns: 'reject' and 'comment'. The 'reject' column will contain either 'True' or 'False'.\n    The 'comment' column will contain the comment string entered into the comment field.\n    Args:\n        ds: DataSet object\n        pdf_path: Path to screening pdf file\n    Returns: DataSet object with checkbox and comment fields added to each DataItem's info\n    \"\"\"\nnew_ds = ds.copy()\n_info_table = new_ds.copy().info_table\ntest_id_key = ds.test_id_key\n# drop reject and comment cols if they exist\nif 'reject' in _info_table.columns:\n_info_table.drop(columns=['reject'], inplace=True)\nif 'comment' in _info_table.columns:\n_info_table.drop(columns=['comment'], inplace=True)\n# dataframe for screening results\nscreening_df = pd.DataFrame(columns=[test_id_key, 'reject', 'comment'])\nwith open(pdf_path, 'rb') as f:\npdf_fields = PdfReader(f).get_fields()\n# get comment and reject fields\ncomment_fields = [field for field in pdf_fields if 'comment' in field]\nreject_fields = [field for field in pdf_fields if 'reject' in field]\n# get test_ids from comment fields\ntest_ids = [field.split('_box_')[1] for field in comment_fields]\n# get comments and rejects\ncomments = [pdf_fields[field]['/V'] for field in comment_fields]\nrejects = [pdf_fields[field]['/V'] for field in reject_fields]\n# add to dataframe\nscreening_df[test_id_key] = test_ids\nscreening_df['reject'] = rejects\nscreening_df['comment'] = comments\n# replace reject /Yes values with True, and /Off with False\nscreening_df['reject'] = screening_df['reject'].replace('/Yes', 'True')\nscreening_df['reject'] = screening_df['reject'].replace('/Off', 'False')\n_info_table = _info_table.merge(screening_df, on=test_id_key, how='left')\nnew_ds.info_table = _info_table\nreturn new_ds\n</code></pre>"},{"location":"reference/screening/#paramaterial.screening.remove_rejected_items","title":"<code>remove_rejected_items(ds, reject_key='reject')</code>","text":"<p>Remove DataItems from the DataSet that were marked as rejected in the screening pdf. DataItems will be removed if the value in the reject_key column of the ds.info_table is 'True'.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>DataSet</code> <p>DataSet object</p> required <code>reject_key</code> <code>str</code> <p>Column name in the info_table that contains the reject values</p> <code>'reject'</code> Source code in <code>paramaterial\\screening.py</code> <pre><code>def remove_rejected_items(ds: DataSet, reject_key: str = 'reject') -&gt; DataSet:\n\"\"\"Remove DataItems from the DataSet that were marked as rejected in the screening pdf.\n    DataItems will be removed if the value in the reject_key column of the ds.info_table is 'True'.\n    Args:\n        ds: DataSet object\n        reject_key: Column name in the info_table that contains the reject values\n    \"\"\"\nnew_ds = ds.copy()\nnew_ds.info_table = new_ds.info_table[new_ds.info_table['reject'] != 'True']\n# print a list of the rejected items with a detailed message\nrejected_items = new_ds.info_table[new_ds.info_table['reject'] == 'True']\nfor i, row in rejected_items.iterrows():\nprint(f'Item {row[new_ds.test_id_key]} was rejected because {row[\"comment\"]}')\nreturn new_ds\n</code></pre>"}]}